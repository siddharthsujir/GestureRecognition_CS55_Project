{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7751653,"sourceType":"datasetVersion","datasetId":4532233}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gesture Recognition\nIn this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started.","metadata":{"id":"W-s0F6P0-htv"}},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom imageio import imread\nfrom skimage.transform import resize\nimport datetime\nimport os","metadata":{"id":"2u0bNQxY-htw","executionInfo":{"status":"ok","timestamp":1709389956420,"user_tz":-330,"elapsed":1387,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:20.033012Z","iopub.execute_input":"2024-03-06T06:59:20.033844Z","iopub.status.idle":"2024-03-06T06:59:20.245884Z","shell.execute_reply.started":"2024-03-06T06:59:20.033813Z","shell.execute_reply":"2024-03-06T06:59:20.244953Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!pip install scipy","metadata":{"id":"oruZ73yz57LH","executionInfo":{"status":"ok","timestamp":1709389956421,"user_tz":-330,"elapsed":4,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:20.247957Z","iopub.execute_input":"2024-03-06T06:59:20.248475Z","iopub.status.idle":"2024-03-06T06:59:20.252314Z","shell.execute_reply.started":"2024-03-06T06:59:20.248443Z","shell.execute_reply":"2024-03-06T06:59:20.251525Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"E0lKWdNb0SVf","executionInfo":{"status":"ok","timestamp":1709389987073,"user_tz":-330,"elapsed":30655,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"outputId":"6c2abda8-68c7-4f57-9b1b-4a63a0fb5dbe","execution":{"iopub.status.busy":"2024-03-06T06:59:20.253492Z","iopub.execute_input":"2024-03-06T06:59:20.253814Z","iopub.status.idle":"2024-03-06T06:59:20.263727Z","shell.execute_reply.started":"2024-03-06T06:59:20.253785Z","shell.execute_reply":"2024-03-06T06:59:20.262808Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#!unzip '/content/drive/MyDrive/Colab Notebooks/Project_data.zip' -d '/content/drive/MyDrive/Colab Notebooks/Project_data'","metadata":{"id":"_poxAEFN4rdb","executionInfo":{"status":"ok","timestamp":1709389987074,"user_tz":-330,"elapsed":8,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:20.265286Z","iopub.execute_input":"2024-03-06T06:59:20.265833Z","iopub.status.idle":"2024-03-06T06:59:20.273536Z","shell.execute_reply.started":"2024-03-06T06:59:20.265791Z","shell.execute_reply":"2024-03-06T06:59:20.272650Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We set the random seed so that the results don't vary drastically.","metadata":{"id":"T5i-mYO4-htw"}},{"cell_type":"code","source":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)","metadata":{"id":"bE09ifSc-htw","executionInfo":{"status":"ok","timestamp":1709389989879,"user_tz":-330,"elapsed":2811,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:20.276062Z","iopub.execute_input":"2024-03-06T06:59:20.276347Z","iopub.status.idle":"2024-03-06T06:59:34.072361Z","shell.execute_reply.started":"2024-03-06T06:59:20.276325Z","shell.execute_reply":"2024-03-06T06:59:34.071384Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-03-06 06:59:22.311227: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-06 06:59:22.311334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-06 06:59:22.491278: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.","metadata":{"id":"u0W2kcBQ-htw"}},{"cell_type":"code","source":"train_doc = np.random.permutation(open('/kaggle/input/gesture-reconition/Project_data/train.csv').readlines())\nval_doc = np.random.permutation(open('/kaggle/input/gesture-reconition/Project_data/val.csv').readlines())\nbatch_size = 10 #experiment with the batch size\n\nidx_seq = range(7,26,2)\nimg_height = 120\nimg_width = 120","metadata":{"id":"_UepaDFF-htw","executionInfo":{"status":"ok","timestamp":1709389993474,"user_tz":-330,"elapsed":3632,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:34.073594Z","iopub.execute_input":"2024-03-06T06:59:34.074588Z","iopub.status.idle":"2024-03-06T06:59:34.096639Z","shell.execute_reply.started":"2024-03-06T06:59:34.074560Z","shell.execute_reply":"2024-03-06T06:59:34.095424Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Generator\nThis is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.","metadata":{"id":"rEKix2qM-htx"}},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size):\n    print('Source path = ', source_path, '; batch size =', batch_size)\n    img_idx = idx_seq # create a list of image numbers you want to use for a particular video\n    while True:\n        t = np.random.permutation(folder_list)\n        num_batches = len(folder_list) // batch_size # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size, 20, img_height, img_width, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size, 5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx, item in enumerate(img_idx): # Iterate over the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    image = resize(image, (img_height, img_width, 3))  # Resize the image to desired dimensions\n\n                    # crop the images and resize them. Note that the images are of 2 different shapes\n                    # and the conv3D will throw an error if the inputs in a batch have different shapes\n\n                    # batch_data[folder, idx, :, :, 0] = (image[:, :, 0] - np.percentile(image[:, :, 0], 5)) / (np.percentile(image[:, :, 0], 95) - np.percentile(image[:, :, 0], 5)) # normalise and feed in the image\n                    # batch_data[folder, idx, :, :, 1] = (image[:, :, 1] - np.percentile(image[:, :, 1], 5)) / (np.percentile(image[:, :, 1], 95) - np.percentile(image[:, :, 1], 5)) # normalise and feed in the image\n                    # batch_data[folder, idx, :, :, 2] = (image[:, :, 2] - np.percentile(image[:, :, 2], 5)) / (np.percentile(image[:, :, 2], 95) - np.percentile(image[:, :, 2], 5)) # normalise and feed in the image\n                    batch_data[folder, idx, :, :, 0] = (image[:, :, 0]) / 255\n                    batch_data[folder, idx, :, :, 1] = (image[:, :, 1]) / 255\n                    batch_data[folder, idx, :, :, 2] = (image[:, :, 2]) / 255\n\n\n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels # you yield the batch_data and the batch_labels, remember what does yield do\n\n        # handle remaining data points after full batches\n        if len(folder_list) % batch_size != 0:\n            remaining_batch_size = len(folder_list) % batch_size\n            batch_data = np.zeros((remaining_batch_size, 20, img_height, img_height, 3))\n            batch_labels = np.zeros((remaining_batch_size, 5))\n            for folder in range(remaining_batch_size):\n                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n                for idx, item in enumerate(img_idx):\n                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    image = resize(image, (img_height, img_width, 3))  # Resize the image to desired dimensions\n                    batch_data[folder, idx, :, :, 0] = (image[:, :, 0]) / 255\n                    batch_data[folder, idx, :, :, 1] = (image[:, :, 1]) / 255\n                    batch_data[folder, idx, :, :, 2] = (image[:, :, 2]) / 255\n\n                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels","metadata":{"id":"MGR3EDqs-htx","executionInfo":{"status":"ok","timestamp":1709389993474,"user_tz":-330,"elapsed":28,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:34.098387Z","iopub.execute_input":"2024-03-06T06:59:34.098745Z","iopub.status.idle":"2024-03-06T06:59:34.117701Z","shell.execute_reply.started":"2024-03-06T06:59:34.098713Z","shell.execute_reply":"2024-03-06T06:59:34.116702Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture.","metadata":{"id":"CtpUo2sL-htx"}},{"cell_type":"code","source":"curr_dt_time = datetime.datetime.now()\ntrain_path = '/kaggle/input/gesture-reconition/Project_data/train'\nval_path = '/kaggle/input/gesture-reconition/Project_data/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 1 # choose the number of epochs\nprint ('# epochs =', num_epochs)\nnum_classes= 5","metadata":{"id":"zW5PTcYc-htx","executionInfo":{"status":"ok","timestamp":1709389993474,"user_tz":-330,"elapsed":12,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"outputId":"fa23a73a-33be-49c6-9403-d67994410848","execution":{"iopub.status.busy":"2024-03-06T06:59:34.118904Z","iopub.execute_input":"2024-03-06T06:59:34.119243Z","iopub.status.idle":"2024-03-06T06:59:34.130947Z","shell.execute_reply.started":"2024-03-06T06:59:34.119183Z","shell.execute_reply":"2024-03-06T06:59:34.129956Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"# training sequences = 663\n# validation sequences = 100\n# epochs = 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model\nHere you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.","metadata":{"id":"YltYzCh4-htx"}},{"cell_type":"markdown","source":"**Model with batch size 20**","metadata":{"id":"H0J5P6eaLV7z"}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv3D, MaxPooling3D, MaxPooling2D, RandomFlip, RandomRotation,Rescaling, LSTM\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n#from keras.layers.convolutional import Conv3D, MaxPooling3D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\n\ndef generate_model(batch_size, img_height, img_width, dropout=0.25, dense=64):\n  #write your model here\n  model = Sequential()\n\n  #model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(batch_size ,img_height, img_width,3)))\n\n  model.add(Conv3D(16, (3, 3, 3), padding='same',input_shape=(batch_size ,img_height, img_width,3)))\n  model.add(Activation('relu'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n  model.add(Conv3D(32, (2, 2, 2), padding='same'))\n  model.add(Activation('relu'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n  model.add(Conv3D(64, (2, 2, 2), padding='same'))\n  model.add(Activation('relu'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n\n  model.add(Conv3D(128, (2, 2, 2), padding='same'))\n  model.add(Activation('relu'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n\n  model.add(Flatten())\n  model.add(Dense(dense,activation='relu'))\n  model.add(BatchNormalization())\n  model.add(Dropout(dropout))\n\n  model.add(Dense(dense,activation='relu'))\n  model.add(BatchNormalization())\n  model.add(Dropout(dropout))\n\n  model.add(Dense(num_classes,activation='softmax'))\n  return model","metadata":{"id":"139lKwop-htx","executionInfo":{"status":"ok","timestamp":1709389993474,"user_tz":-330,"elapsed":7,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:34.132207Z","iopub.execute_input":"2024-03-06T06:59:34.132537Z","iopub.status.idle":"2024-03-06T06:59:34.270004Z","shell.execute_reply.started":"2024-03-06T06:59:34.132515Z","shell.execute_reply":"2024-03-06T06:59:34.269330Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.","metadata":{"id":"SI_MDo4T-htx"}},{"cell_type":"code","source":"model = generate_model(batch_size, img_height, img_width, 0.25, 128)","metadata":{"id":"at3qP3BXN12k","executionInfo":{"status":"ok","timestamp":1709389994315,"user_tz":-330,"elapsed":847,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:34.271058Z","iopub.execute_input":"2024-03-06T06:59:34.271369Z","iopub.status.idle":"2024-03-06T06:59:35.411777Z","shell.execute_reply.started":"2024-03-06T06:59:34.271346Z","shell.execute_reply":"2024-03-06T06:59:35.410778Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"optimiser = 'adam' #write your optimizer\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","metadata":{"scrolled":true,"id":"k80icMMo-htx","executionInfo":{"status":"ok","timestamp":1709389995295,"user_tz":-330,"elapsed":990,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"outputId":"8b166483-959d-40f2-e093-da85f07a4e41","execution":{"iopub.status.busy":"2024-03-06T06:59:35.413008Z","iopub.execute_input":"2024-03-06T06:59:35.413305Z","iopub.status.idle":"2024-03-06T06:59:35.490053Z","shell.execute_reply.started":"2024-03-06T06:59:35.413281Z","shell.execute_reply":"2024-03-06T06:59:35.487852Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv3d (Conv3D)             (None, 10, 120, 120, 16   1312      \n                             )                                   \n                                                                 \n activation (Activation)     (None, 10, 120, 120, 16   0         \n                             )                                   \n                                                                 \n batch_normalization (Batch  (None, 10, 120, 120, 16   64        \n Normalization)              )                                   \n                                                                 \n max_pooling3d (MaxPooling3  (None, 5, 60, 60, 16)     0         \n D)                                                              \n                                                                 \n conv3d_1 (Conv3D)           (None, 5, 60, 60, 32)     4128      \n                                                                 \n activation_1 (Activation)   (None, 5, 60, 60, 32)     0         \n                                                                 \n batch_normalization_1 (Bat  (None, 5, 60, 60, 32)     128       \n chNormalization)                                                \n                                                                 \n max_pooling3d_1 (MaxPoolin  (None, 2, 30, 30, 32)     0         \n g3D)                                                            \n                                                                 \n conv3d_2 (Conv3D)           (None, 2, 30, 30, 64)     16448     \n                                                                 \n activation_2 (Activation)   (None, 2, 30, 30, 64)     0         \n                                                                 \n batch_normalization_2 (Bat  (None, 2, 30, 30, 64)     256       \n chNormalization)                                                \n                                                                 \n max_pooling3d_2 (MaxPoolin  (None, 1, 15, 15, 64)     0         \n g3D)                                                            \n                                                                 \n conv3d_3 (Conv3D)           (None, 1, 15, 15, 128)    65664     \n                                                                 \n activation_3 (Activation)   (None, 1, 15, 15, 128)    0         \n                                                                 \n batch_normalization_3 (Bat  (None, 1, 15, 15, 128)    512       \n chNormalization)                                                \n                                                                 \n max_pooling3d_3 (MaxPoolin  (None, 1, 8, 8, 128)      0         \n g3D)                                                            \n                                                                 \n flatten (Flatten)           (None, 8192)              0         \n                                                                 \n dense (Dense)               (None, 128)               1048704   \n                                                                 \n batch_normalization_4 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 128)               16512     \n                                                                 \n batch_normalization_5 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_2 (Dense)             (None, 5)                 645       \n                                                                 \n=================================================================\nTotal params: 1155397 (4.41 MB)\nTrainable params: 1154405 (4.40 MB)\nNon-trainable params: 992 (3.88 KB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.","metadata":{"id":"-ZXfyZFg-htx"}},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"id":"mndig58e-htx","executionInfo":{"status":"ok","timestamp":1709389995295,"user_tz":-330,"elapsed":118,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.491186Z","iopub.execute_input":"2024-03-06T06:59:35.491475Z","iopub.status.idle":"2024-03-06T06:59:35.495904Z","shell.execute_reply.started":"2024-03-06T06:59:35.491451Z","shell.execute_reply":"2024-03-06T06:59:35.495024Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n\nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n\nfilepath = '/kaggle/working/' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nfrom keras.callbacks import ReduceLROnPlateau\n\nLR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n\ncallbacks_list = [checkpoint, LR]","metadata":{"id":"AHzEvYlB-hty","executionInfo":{"status":"ok","timestamp":1709389995295,"user_tz":-330,"elapsed":117,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"outputId":"7be30e08-ba2c-406e-889b-15274481ef16","execution":{"iopub.status.busy":"2024-03-06T06:59:35.496993Z","iopub.execute_input":"2024-03-06T06:59:35.497244Z","iopub.status.idle":"2024-03-06T06:59:35.506388Z","shell.execute_reply.started":"2024-03-06T06:59:35.497222Z","shell.execute_reply":"2024-03-06T06:59:35.505145Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.","metadata":{"id":"k-D-Op5b-hty"}},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"id":"2PLmBUlF-hty","executionInfo":{"status":"ok","timestamp":1709389995296,"user_tz":-330,"elapsed":112,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.510413Z","iopub.execute_input":"2024-03-06T06:59:35.510734Z","iopub.status.idle":"2024-03-06T06:59:35.516741Z","shell.execute_reply.started":"2024-03-06T06:59:35.510712Z","shell.execute_reply":"2024-03-06T06:59:35.515726Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.","metadata":{"id":"Hfupo2kK-hty"}},{"cell_type":"code","source":"#model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"id":"iHOwcouq-hty","executionInfo":{"status":"ok","timestamp":1709389995296,"user_tz":-330,"elapsed":112,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.517804Z","iopub.execute_input":"2024-03-06T06:59:35.518139Z","iopub.status.idle":"2024-03-06T06:59:35.525763Z","shell.execute_reply.started":"2024-03-06T06:59:35.518073Z","shell.execute_reply":"2024-03-06T06:59:35.524677Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Validation accuracy = 0.44\nTraining accuracy = 0.16\n\nAccuracy is not that great. Also \n\n**There is clear sign of overfitting as the difference between the training accuracy and validation accuracy is more. **","metadata":{}},{"cell_type":"markdown","source":"Source path =  /kaggle/input/gesture-reconition/Project_data/train ; batch size = 10\n/tmp/ipykernel_34/3799537928.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n/tmp/ipykernel_34/515647629.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1709479767.022459     229 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n66/67 [============================>.] - ETA: 1s - loss: 1.5532 - categorical_accuracy: 0.4348\n/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n67/67 [==============================] - ETA: 0s - loss: 1.5532 - categorical_accuracy: 0.4359Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 10\n\nEpoch 1: saving model to model_init_2024-03-0315_29_16.612602/model-00001-1.55318-0.43590-6.49745-0.16000.h5\n67/67 [==============================] - 136s 2s/step - loss: 1.5532 - categorical_accuracy: 0.4359 - val_loss: 6.4975 - val_categorical_accuracy: 0.1600 - lr: 0.0010","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"8gTr0rGO-hty","executionInfo":{"status":"ok","timestamp":1709389995296,"user_tz":-330,"elapsed":111,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model with Batch size 20 num of epocs 10**","metadata":{"id":"yZS4jLqpLIIF"}},{"cell_type":"code","source":"batch_size = 20\nnum_epochs = 10\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"id":"Axv7hzj1OJLw","executionInfo":{"status":"ok","timestamp":1709389995296,"user_tz":-330,"elapsed":111,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.527166Z","iopub.execute_input":"2024-03-06T06:59:35.527528Z","iopub.status.idle":"2024-03-06T06:59:35.535934Z","shell.execute_reply.started":"2024-03-06T06:59:35.527466Z","shell.execute_reply":"2024-03-06T06:59:35.534756Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"id":"f58VbdHPOJSA","executionInfo":{"status":"ok","timestamp":1709389995297,"user_tz":-330,"elapsed":111,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.537550Z","iopub.execute_input":"2024-03-06T06:59:35.537959Z","iopub.status.idle":"2024-03-06T06:59:35.545032Z","shell.execute_reply.started":"2024-03-06T06:59:35.537929Z","shell.execute_reply":"2024-03-06T06:59:35.544014Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model_2 = generate_model(batch_size, img_height, img_width, 0.25, 64)","metadata":{"id":"l830b5B2Lfp0","executionInfo":{"status":"ok","timestamp":1709389995297,"user_tz":-330,"elapsed":111,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.546314Z","iopub.execute_input":"2024-03-06T06:59:35.546986Z","iopub.status.idle":"2024-03-06T06:59:35.787328Z","shell.execute_reply.started":"2024-03-06T06:59:35.546962Z","shell.execute_reply":"2024-03-06T06:59:35.786430Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"optimiser = 'adam' #write your optimizer\nmodel_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_2.summary())","metadata":{"id":"VJxlaO0SPITn","executionInfo":{"status":"ok","timestamp":1709389995297,"user_tz":-330,"elapsed":110,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"outputId":"48fa1db3-112a-4af6-ac18-9cad12efb6ea","execution":{"iopub.status.busy":"2024-03-06T06:59:35.788652Z","iopub.execute_input":"2024-03-06T06:59:35.789505Z","iopub.status.idle":"2024-03-06T06:59:35.863135Z","shell.execute_reply.started":"2024-03-06T06:59:35.789465Z","shell.execute_reply":"2024-03-06T06:59:35.862156Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv3d_4 (Conv3D)           (None, 20, 120, 120, 16   1312      \n                             )                                   \n                                                                 \n activation_4 (Activation)   (None, 20, 120, 120, 16   0         \n                             )                                   \n                                                                 \n batch_normalization_6 (Bat  (None, 20, 120, 120, 16   64        \n chNormalization)            )                                   \n                                                                 \n max_pooling3d_4 (MaxPoolin  (None, 10, 60, 60, 16)    0         \n g3D)                                                            \n                                                                 \n conv3d_5 (Conv3D)           (None, 10, 60, 60, 32)    4128      \n                                                                 \n activation_5 (Activation)   (None, 10, 60, 60, 32)    0         \n                                                                 \n batch_normalization_7 (Bat  (None, 10, 60, 60, 32)    128       \n chNormalization)                                                \n                                                                 \n max_pooling3d_5 (MaxPoolin  (None, 5, 30, 30, 32)     0         \n g3D)                                                            \n                                                                 \n conv3d_6 (Conv3D)           (None, 5, 30, 30, 64)     16448     \n                                                                 \n activation_6 (Activation)   (None, 5, 30, 30, 64)     0         \n                                                                 \n batch_normalization_8 (Bat  (None, 5, 30, 30, 64)     256       \n chNormalization)                                                \n                                                                 \n max_pooling3d_6 (MaxPoolin  (None, 2, 15, 15, 64)     0         \n g3D)                                                            \n                                                                 \n conv3d_7 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n                                                                 \n activation_7 (Activation)   (None, 2, 15, 15, 128)    0         \n                                                                 \n batch_normalization_9 (Bat  (None, 2, 15, 15, 128)    512       \n chNormalization)                                                \n                                                                 \n max_pooling3d_7 (MaxPoolin  (None, 1, 8, 8, 128)      0         \n g3D)                                                            \n                                                                 \n flatten_1 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_3 (Dense)             (None, 64)                524352    \n                                                                 \n batch_normalization_10 (Ba  (None, 64)                256       \n tchNormalization)                                               \n                                                                 \n dropout_2 (Dropout)         (None, 64)                0         \n                                                                 \n dense_4 (Dense)             (None, 64)                4160      \n                                                                 \n batch_normalization_11 (Ba  (None, 64)                256       \n tchNormalization)                                               \n                                                                 \n dropout_3 (Dropout)         (None, 64)                0         \n                                                                 \n dense_5 (Dense)             (None, 5)                 325       \n                                                                 \n=================================================================\nTotal params: 617861 (2.36 MB)\nTrainable params: 617125 (2.35 MB)\nNon-trainable params: 736 (2.88 KB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"#model_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"id":"dfVnchjIO2q2","executionInfo":{"status":"ok","timestamp":1709389995297,"user_tz":-330,"elapsed":14,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.864531Z","iopub.execute_input":"2024-03-06T06:59:35.865128Z","iopub.status.idle":"2024-03-06T06:59:35.869316Z","shell.execute_reply.started":"2024-03-06T06:59:35.865076Z","shell.execute_reply":"2024-03-06T06:59:35.868391Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Source path =  /kaggle/input/gesture-reconition/Project_data/train ; batch size = 20\n/tmp/ipykernel_34/1443121102.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  model_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n/tmp/ipykernel_34/515647629.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\nEpoch 1/10\n33/34 [============================>.] - ETA: 2s - loss: 1.4936 - categorical_accuracy: 0.4561\n/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n34/34 [==============================] - ETA: 0s - loss: 1.4920 - categorical_accuracy: 0.4555Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 20\n\nEpoch 1: saving model to model_init_2024-03-0315_29_16.612602/model-00001-1.49202-0.45551-8.32596-0.16000.h5\n34/34 [==============================] - 96s 3s/step - loss: 1.4920 - categorical_accuracy: 0.4555 - val_loss: 8.3260 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 2/10\n34/34 [==============================] - ETA: 0s - loss: 0.9696 - categorical_accuracy: 0.5988\nEpoch 2: saving model to model_init_2024-03-0315_29_16.612602/model-00002-0.96960-0.59879-19.08339-0.15000.h5\n34/34 [==============================] - 89s 3s/step - loss: 0.9696 - categorical_accuracy: 0.5988 - val_loss: 19.0834 - val_categorical_accuracy: 0.1500 - lr: 0.0010\nEpoch 3/10\n34/34 [==============================] - ETA: 0s - loss: 0.8006 - categorical_accuracy: 0.7014\nEpoch 3: saving model to model_init_2024-03-0315_29_16.612602/model-00003-0.80058-0.70136-24.73187-0.16000.h5\n34/34 [==============================] - 85s 3s/step - loss: 0.8006 - categorical_accuracy: 0.7014 - val_loss: 24.7319 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 4/10\n34/34 [==============================] - ETA: 0s - loss: 0.7110 - categorical_accuracy: 0.7360\nEpoch 4: saving model to model_init_2024-03-0315_29_16.612602/model-00004-0.71102-0.73605-33.21298-0.11000.h5\n34/34 [==============================] - 81s 2s/step - loss: 0.7110 - categorical_accuracy: 0.7360 - val_loss: 33.2130 - val_categorical_accuracy: 0.1100 - lr: 0.0010\nEpoch 5/10\n34/34 [==============================] - ETA: 0s - loss: 0.7553 - categorical_accuracy: 0.7376\nEpoch 5: saving model to model_init_2024-03-0315_29_16.612602/model-00005-0.75534-0.73756-25.44184-0.15000.h5\n34/34 [==============================] - 81s 2s/step - loss: 0.7553 - categorical_accuracy: 0.7376 - val_loss: 25.4418 - val_categorical_accuracy: 0.1500 - lr: 0.0010\nEpoch 6/10\n34/34 [==============================] - ETA: 0s - loss: 0.6312 - categorical_accuracy: 0.7707\nEpoch 6: saving model to model_init_2024-03-0315_29_16.612602/model-00006-0.63116-0.77074-16.28077-0.16000.h5\n34/34 [==============================] - 80s 2s/step - loss: 0.6312 - categorical_accuracy: 0.7707 - val_loss: 16.2808 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 7/10\n34/34 [==============================] - ETA: 0s - loss: 0.5374 - categorical_accuracy: 0.8100\nEpoch 7: saving model to model_init_2024-03-0315_29_16.612602/model-00007-0.53740-0.80995-12.35917-0.17000.h5\n34/34 [==============================] - 78s 2s/step - loss: 0.5374 - categorical_accuracy: 0.8100 - val_loss: 12.3592 - val_categorical_accuracy: 0.1700 - lr: 0.0010\nEpoch 8/10\n34/34 [==============================] - ETA: 0s - loss: 0.5046 - categorical_accuracy: 0.8115\nEpoch 8: saving model to model_init_2024-03-0315_29_16.612602/model-00008-0.50459-0.81146-10.81409-0.13000.h5\n34/34 [==============================] - 79s 2s/step - loss: 0.5046 - categorical_accuracy: 0.8115 - val_loss: 10.8141 - val_categorical_accuracy: 0.1300 - lr: 0.0010\nEpoch 9/10\n34/34 [==============================] - ETA: 0s - loss: 0.4961 - categorical_accuracy: 0.8069\nEpoch 9: saving model to model_init_2024-03-0315_29_16.612602/model-00009-0.49606-0.80694-11.32586-0.17000.h5\n34/34 [==============================] - 80s 2s/step - loss: 0.4961 - categorical_accuracy: 0.8069 - val_loss: 11.3259 - val_categorical_accuracy: 0.1700 - lr: 0.0010\nEpoch 10/10\n34/34 [==============================] - ETA: 0s - loss: 0.4405 - categorical_accuracy: 0.8386\nEpoch 10: saving model to model_init_2024-03-0315_29_16.612602/model-00010-0.44049-0.83861-7.41945-0.17000.h5\n34/34 [==============================] - 80s 2s/step - loss: 0.4405 - categorical_accuracy: 0.8386 - val_loss: 7.4195 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n<keras.src.callbacks.History at 0x7f9af809e1a0>","metadata":{}},{"cell_type":"markdown","source":"**There is clear overfitting in the above model due to a large difference between training and Validation accuracy**","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"XE318rCfRMEj","executionInfo":{"status":"ok","timestamp":1709389995297,"user_tz":-330,"elapsed":13,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 3 - Batch size 15, epochs 30, drop out 0.25 and dense 64","metadata":{}},{"cell_type":"code","source":"batch_size = 15\nnum_epochs = 30\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"id":"jhgkrXWjRMZT","executionInfo":{"status":"ok","timestamp":1709389995297,"user_tz":-330,"elapsed":13,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.870861Z","iopub.execute_input":"2024-03-06T06:59:35.871229Z","iopub.status.idle":"2024-03-06T06:59:35.879614Z","shell.execute_reply.started":"2024-03-06T06:59:35.871196Z","shell.execute_reply":"2024-03-06T06:59:35.878561Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"id":"s_QhRIr3RMc7","executionInfo":{"status":"ok","timestamp":1709389995298,"user_tz":-330,"elapsed":14,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.880807Z","iopub.execute_input":"2024-03-06T06:59:35.881184Z","iopub.status.idle":"2024-03-06T06:59:35.889449Z","shell.execute_reply.started":"2024-03-06T06:59:35.881130Z","shell.execute_reply":"2024-03-06T06:59:35.888477Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_3 = generate_model(batch_size, img_height, img_width, 0.25, 64)","metadata":{"id":"8ohJ91tvRMgS","executionInfo":{"status":"ok","timestamp":1709389995298,"user_tz":-330,"elapsed":12,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"execution":{"iopub.status.busy":"2024-03-06T06:59:35.890643Z","iopub.execute_input":"2024-03-06T06:59:35.891015Z","iopub.status.idle":"2024-03-06T06:59:36.293355Z","shell.execute_reply.started":"2024-03-06T06:59:35.890989Z","shell.execute_reply":"2024-03-06T06:59:36.292591Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"optimiser = 'adam' #write your optimizer\nmodel_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_3.summary())","metadata":{"id":"szVFh3bURaRk","executionInfo":{"status":"ok","timestamp":1709389996092,"user_tz":-330,"elapsed":34,"user":{"displayName":"Siddharth Sujir","userId":"06083318014128150583"}},"outputId":"124d5085-3f7e-4fbe-ac2f-8007eb88d17d","execution":{"iopub.status.busy":"2024-03-06T06:59:36.294551Z","iopub.execute_input":"2024-03-06T06:59:36.294904Z","iopub.status.idle":"2024-03-06T06:59:36.363402Z","shell.execute_reply.started":"2024-03-06T06:59:36.294873Z","shell.execute_reply":"2024-03-06T06:59:36.362540Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv3d_8 (Conv3D)           (None, 15, 120, 120, 16   1312      \n                             )                                   \n                                                                 \n activation_8 (Activation)   (None, 15, 120, 120, 16   0         \n                             )                                   \n                                                                 \n batch_normalization_12 (Ba  (None, 15, 120, 120, 16   64        \n tchNormalization)           )                                   \n                                                                 \n max_pooling3d_8 (MaxPoolin  (None, 7, 60, 60, 16)     0         \n g3D)                                                            \n                                                                 \n conv3d_9 (Conv3D)           (None, 7, 60, 60, 32)     4128      \n                                                                 \n activation_9 (Activation)   (None, 7, 60, 60, 32)     0         \n                                                                 \n batch_normalization_13 (Ba  (None, 7, 60, 60, 32)     128       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_9 (MaxPoolin  (None, 3, 30, 30, 32)     0         \n g3D)                                                            \n                                                                 \n conv3d_10 (Conv3D)          (None, 3, 30, 30, 64)     16448     \n                                                                 \n activation_10 (Activation)  (None, 3, 30, 30, 64)     0         \n                                                                 \n batch_normalization_14 (Ba  (None, 3, 30, 30, 64)     256       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_10 (MaxPooli  (None, 1, 15, 15, 64)     0         \n ng3D)                                                           \n                                                                 \n conv3d_11 (Conv3D)          (None, 1, 15, 15, 128)    65664     \n                                                                 \n activation_11 (Activation)  (None, 1, 15, 15, 128)    0         \n                                                                 \n batch_normalization_15 (Ba  (None, 1, 15, 15, 128)    512       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_11 (MaxPooli  (None, 1, 8, 8, 128)      0         \n ng3D)                                                           \n                                                                 \n flatten_2 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_6 (Dense)             (None, 64)                524352    \n                                                                 \n batch_normalization_16 (Ba  (None, 64)                256       \n tchNormalization)                                               \n                                                                 \n dropout_4 (Dropout)         (None, 64)                0         \n                                                                 \n dense_7 (Dense)             (None, 64)                4160      \n                                                                 \n batch_normalization_17 (Ba  (None, 64)                256       \n tchNormalization)                                               \n                                                                 \n dropout_5 (Dropout)         (None, 64)                0         \n                                                                 \n dense_8 (Dense)             (None, 5)                 325       \n                                                                 \n=================================================================\nTotal params: 617861 (2.36 MB)\nTrainable params: 617125 (2.35 MB)\nNon-trainable params: 736 (2.88 KB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"id":"SdfCVn0pRab8","outputId":"d2035fbc-8c15-494e-b219-82f092266483","execution":{"iopub.status.busy":"2024-03-06T07:00:05.495567Z","iopub.execute_input":"2024-03-06T07:00:05.496173Z","iopub.status.idle":"2024-03-06T07:35:33.345532Z","shell.execute_reply.started":"2024-03-06T07:00:05.496135Z","shell.execute_reply":"2024-03-06T07:35:33.344534Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3490547937.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  model_3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n/tmp/ipykernel_34/515647629.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"Source path =  /kaggle/input/gesture-reconition/Project_data/train ; batch size = 15\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1709708413.991369     113 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"44/45 [============================>.] - ETA: 1s - loss: 1.5095 - categorical_accuracy: 0.4182","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"45/45 [==============================] - ETA: 0s - loss: 1.5135 - categorical_accuracy: 0.4163Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 15\n\nEpoch 1: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00001-1.51352-0.41629-8.48297-0.16000.h5\n45/45 [==============================] - 108s 2s/step - loss: 1.5135 - categorical_accuracy: 0.4163 - val_loss: 8.4830 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"45/45 [==============================] - ETA: 0s - loss: 1.0780 - categorical_accuracy: 0.5867\nEpoch 2: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00002-1.07799-0.58673-13.90831-0.13000.h5\n45/45 [==============================] - 76s 2s/step - loss: 1.0780 - categorical_accuracy: 0.5867 - val_loss: 13.9083 - val_categorical_accuracy: 0.1300 - lr: 0.0010\nEpoch 3/30\n45/45 [==============================] - ETA: 0s - loss: 1.1999 - categorical_accuracy: 0.5385\nEpoch 3: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00003-1.19994-0.53846-10.64747-0.18000.h5\n45/45 [==============================] - 79s 2s/step - loss: 1.1999 - categorical_accuracy: 0.5385 - val_loss: 10.6475 - val_categorical_accuracy: 0.1800 - lr: 0.0010\nEpoch 4/30\n45/45 [==============================] - ETA: 0s - loss: 0.9741 - categorical_accuracy: 0.6229\nEpoch 4: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00004-0.97415-0.62293-7.08547-0.21000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.9741 - categorical_accuracy: 0.6229 - val_loss: 7.0855 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 5/30\n45/45 [==============================] - ETA: 0s - loss: 0.7932 - categorical_accuracy: 0.6998\nEpoch 5: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00005-0.79316-0.69985-7.87959-0.27000.h5\n45/45 [==============================] - 68s 2s/step - loss: 0.7932 - categorical_accuracy: 0.6998 - val_loss: 7.8796 - val_categorical_accuracy: 0.2700 - lr: 0.0010\nEpoch 6/30\n45/45 [==============================] - ETA: 0s - loss: 0.7723 - categorical_accuracy: 0.6938\nEpoch 6: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00006-0.77232-0.69382-8.25626-0.21000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.7723 - categorical_accuracy: 0.6938 - val_loss: 8.2563 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 7/30\n45/45 [==============================] - ETA: 0s - loss: 0.6181 - categorical_accuracy: 0.7828\nEpoch 7: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00007-0.61806-0.78281-7.34590-0.19000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.6181 - categorical_accuracy: 0.7828 - val_loss: 7.3459 - val_categorical_accuracy: 0.1900 - lr: 0.0010\nEpoch 8/30\n45/45 [==============================] - ETA: 0s - loss: 0.4927 - categorical_accuracy: 0.8311\nEpoch 8: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00008-0.49275-0.83107-6.16196-0.21000.h5\n45/45 [==============================] - 66s 1s/step - loss: 0.4927 - categorical_accuracy: 0.8311 - val_loss: 6.1620 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 9/30\n45/45 [==============================] - ETA: 0s - loss: 0.5717 - categorical_accuracy: 0.7979\nEpoch 9: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00009-0.57172-0.79789-5.14809-0.22000.h5\n45/45 [==============================] - 69s 2s/step - loss: 0.5717 - categorical_accuracy: 0.7979 - val_loss: 5.1481 - val_categorical_accuracy: 0.2200 - lr: 0.0010\nEpoch 10/30\n45/45 [==============================] - ETA: 0s - loss: 0.4311 - categorical_accuracy: 0.8462\nEpoch 10: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00010-0.43111-0.84615-5.98527-0.23000.h5\n45/45 [==============================] - 75s 2s/step - loss: 0.4311 - categorical_accuracy: 0.8462 - val_loss: 5.9853 - val_categorical_accuracy: 0.2300 - lr: 0.0010\nEpoch 11/30\n45/45 [==============================] - ETA: 0s - loss: 0.3654 - categorical_accuracy: 0.8748\nEpoch 11: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00011-0.36539-0.87481-4.45609-0.21000.h5\n45/45 [==============================] - 89s 2s/step - loss: 0.3654 - categorical_accuracy: 0.8748 - val_loss: 4.4561 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 12/30\n45/45 [==============================] - ETA: 0s - loss: 0.2912 - categorical_accuracy: 0.8914\nEpoch 12: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00012-0.29118-0.89140-2.08685-0.34000.h5\n45/45 [==============================] - 101s 2s/step - loss: 0.2912 - categorical_accuracy: 0.8914 - val_loss: 2.0869 - val_categorical_accuracy: 0.3400 - lr: 0.0010\nEpoch 13/30\n45/45 [==============================] - ETA: 0s - loss: 0.2622 - categorical_accuracy: 0.9140\nEpoch 13: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00013-0.26215-0.91403-1.29357-0.58000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.2622 - categorical_accuracy: 0.9140 - val_loss: 1.2936 - val_categorical_accuracy: 0.5800 - lr: 0.0010\nEpoch 14/30\n45/45 [==============================] - ETA: 0s - loss: 0.4147 - categorical_accuracy: 0.8462\nEpoch 14: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00014-0.41474-0.84615-3.95084-0.33000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.4147 - categorical_accuracy: 0.8462 - val_loss: 3.9508 - val_categorical_accuracy: 0.3300 - lr: 0.0010\nEpoch 15/30\n45/45 [==============================] - ETA: 0s - loss: 0.5029 - categorical_accuracy: 0.8024\nEpoch 15: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00015-0.50290-0.80241-2.66469-0.42000.h5\n45/45 [==============================] - 56s 1s/step - loss: 0.5029 - categorical_accuracy: 0.8024 - val_loss: 2.6647 - val_categorical_accuracy: 0.4200 - lr: 0.0010\nEpoch 16/30\n45/45 [==============================] - ETA: 0s - loss: 0.6909 - categorical_accuracy: 0.7481\nEpoch 16: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00016-0.69091-0.74811-1.46579-0.60000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.6909 - categorical_accuracy: 0.7481 - val_loss: 1.4658 - val_categorical_accuracy: 0.6000 - lr: 0.0010\nEpoch 17/30\n45/45 [==============================] - ETA: 0s - loss: 0.5122 - categorical_accuracy: 0.7964\nEpoch 17: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00017-0.51225-0.79638-1.84033-0.46000.h5\n45/45 [==============================] - 57s 1s/step - loss: 0.5122 - categorical_accuracy: 0.7964 - val_loss: 1.8403 - val_categorical_accuracy: 0.4600 - lr: 0.0010\nEpoch 18/30\n45/45 [==============================] - ETA: 0s - loss: 0.4187 - categorical_accuracy: 0.8477\nEpoch 18: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00018-0.41869-0.84766-1.63667-0.47000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.4187 - categorical_accuracy: 0.8477 - val_loss: 1.6367 - val_categorical_accuracy: 0.4700 - lr: 0.0010\nEpoch 19/30\n45/45 [==============================] - ETA: 0s - loss: 0.2542 - categorical_accuracy: 0.8989\nEpoch 19: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00019-0.25423-0.89894-0.85994-0.69000.h5\n45/45 [==============================] - 66s 1s/step - loss: 0.2542 - categorical_accuracy: 0.8989 - val_loss: 0.8599 - val_categorical_accuracy: 0.6900 - lr: 0.0010\nEpoch 20/30\n45/45 [==============================] - ETA: 0s - loss: 0.3023 - categorical_accuracy: 0.8929\nEpoch 20: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00020-0.30227-0.89291-1.43540-0.56000.h5\n45/45 [==============================] - 63s 1s/step - loss: 0.3023 - categorical_accuracy: 0.8929 - val_loss: 1.4354 - val_categorical_accuracy: 0.5600 - lr: 0.0010\nEpoch 21/30\n45/45 [==============================] - ETA: 0s - loss: 0.2586 - categorical_accuracy: 0.9170\nEpoch 21: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00021-0.25857-0.91704-1.71297-0.48000.h5\n45/45 [==============================] - 63s 1s/step - loss: 0.2586 - categorical_accuracy: 0.9170 - val_loss: 1.7130 - val_categorical_accuracy: 0.4800 - lr: 0.0010\nEpoch 22/30\n45/45 [==============================] - ETA: 0s - loss: 0.4110 - categorical_accuracy: 0.8462\nEpoch 22: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00022-0.41095-0.84615-1.71543-0.54000.h5\n45/45 [==============================] - 64s 1s/step - loss: 0.4110 - categorical_accuracy: 0.8462 - val_loss: 1.7154 - val_categorical_accuracy: 0.5400 - lr: 0.0010\nEpoch 23/30\n45/45 [==============================] - ETA: 0s - loss: 0.2352 - categorical_accuracy: 0.9095\nEpoch 23: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00023-0.23517-0.90950-1.16344-0.65000.h5\n45/45 [==============================] - 71s 2s/step - loss: 0.2352 - categorical_accuracy: 0.9095 - val_loss: 1.1634 - val_categorical_accuracy: 0.6500 - lr: 0.0010\nEpoch 24/30\n45/45 [==============================] - ETA: 0s - loss: 0.3124 - categorical_accuracy: 0.9005\nEpoch 24: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00024-0.31241-0.90045-2.50398-0.41000.h5\n45/45 [==============================] - 65s 1s/step - loss: 0.3124 - categorical_accuracy: 0.9005 - val_loss: 2.5040 - val_categorical_accuracy: 0.4100 - lr: 0.0010\nEpoch 25/30\n45/45 [==============================] - ETA: 0s - loss: 0.2357 - categorical_accuracy: 0.9186\nEpoch 25: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00025-0.23571-0.91855-1.25866-0.61000.h5\n45/45 [==============================] - 58s 1s/step - loss: 0.2357 - categorical_accuracy: 0.9186 - val_loss: 1.2587 - val_categorical_accuracy: 0.6100 - lr: 0.0010\nEpoch 26/30\n45/45 [==============================] - ETA: 0s - loss: 0.2021 - categorical_accuracy: 0.9321\nEpoch 26: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00026-0.20215-0.93213-1.78875-0.46000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.2021 - categorical_accuracy: 0.9321 - val_loss: 1.7888 - val_categorical_accuracy: 0.4600 - lr: 0.0010\nEpoch 27/30\n45/45 [==============================] - ETA: 0s - loss: 0.1421 - categorical_accuracy: 0.9487\nEpoch 27: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00027-0.14214-0.94872-0.67279-0.74000.h5\n45/45 [==============================] - 57s 1s/step - loss: 0.1421 - categorical_accuracy: 0.9487 - val_loss: 0.6728 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 28/30\n45/45 [==============================] - ETA: 0s - loss: 0.1622 - categorical_accuracy: 0.9457\nEpoch 28: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00028-0.16220-0.94570-0.97732-0.70000.h5\n45/45 [==============================] - 67s 2s/step - loss: 0.1622 - categorical_accuracy: 0.9457 - val_loss: 0.9773 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 29/30\n45/45 [==============================] - ETA: 0s - loss: 0.1255 - categorical_accuracy: 0.9593\nEpoch 29: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00029-0.12547-0.95928-2.34213-0.55000.h5\n45/45 [==============================] - 64s 1s/step - loss: 0.1255 - categorical_accuracy: 0.9593 - val_loss: 2.3421 - val_categorical_accuracy: 0.5500 - lr: 0.0010\nEpoch 30/30\n45/45 [==============================] - ETA: 0s - loss: 0.3021 - categorical_accuracy: 0.8989\nEpoch 30: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00030-0.30206-0.89894-1.18512-0.70000.h5\n45/45 [==============================] - 63s 1s/step - loss: 0.3021 - categorical_accuracy: 0.8989 - val_loss: 1.1851 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7f05b8d8a740>"},"metadata":{}}]},{"cell_type":"markdown","source":"Source path = /content/drive/MyDrive/Colab NotebooksEpoch 1/30\n44/45 [============================>.] - ETA: 1s - loss: 1.5154 - categorical_accuracy: 0.4288\n/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n45/45 [==============================] - ETA: 0s - loss: 1.5175 - categorical_accuracy: 0.4284Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 15\n\nEpoch 1: saving model to model_init_2024-03-0315_29_16.612602/model-00001-1.51751-0.42836-3.96369-0.19000.h5\n45/45 [==============================] - 98s 2s/step - loss: 1.5175 - categorical_accuracy: 0.4284 - val_loss: 3.9637 - val_categorical_accuracy: 0.1900 - lr: 0.0010\nEpoch 2/30\n45/45 [==============================] - ETA: 0s - loss: 1.1068 - categorical_accuracy: 0.5686\nEpoch 2: saving model to model_init_2024-03-0315_29_16.612602/model-00002-1.10676-0.56863-9.09869-0.16000.h5\n45/45 [==============================] - 94s 2s/step - loss: 1.1068 - categorical_accuracy: 0.5686 - val_loss: 9.0987 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 3/30\n45/45 [==============================] - ETA: 0s - loss: 0.9209 - categorical_accuracy: 0.6410\nEpoch 3: saving model to model_init_2024-03-0315_29_16.612602/model-00003-0.92094-0.64103-9.00184-0.16000.h5\n45/45 [==============================] - 84s 2s/step - loss: 0.9209 - categorical_accuracy: 0.6410 - val_loss: 9.0018 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 4/30\n45/45 [==============================] - ETA: 0s - loss: 0.7995 - categorical_accuracy: 0.6802\nEpoch 4: saving model to model_init_2024-03-0315_29_16.612602/model-00004-0.79947-0.68024-8.35011-0.19000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.7995 - categorical_accuracy: 0.6802 - val_loss: 8.3501 - val_categorical_accuracy: 0.1900 - lr: 0.0010\nEpoch 5/30\n45/45 [==============================] - ETA: 0s - loss: 0.6905 - categorical_accuracy: 0.7300\nEpoch 5: saving model to model_init_2024-03-0315_29_16.612602/model-00005-0.69049-0.73002-9.37489-0.15000.h5\n45/45 [==============================] - 82s 2s/step - loss: 0.6905 - categorical_accuracy: 0.7300 - val_loss: 9.3749 - val_categorical_accuracy: 0.1500 - lr: 0.0010\nEpoch 6/30\n45/45 [==============================] - ETA: 0s - loss: 0.6135 - categorical_accuracy: 0.7677\nEpoch 6: saving model to model_init_2024-03-0315_29_16.612602/model-00006-0.61346-0.76772-10.84826-0.16000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.6135 - categorical_accuracy: 0.7677 - val_loss: 10.8483 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 7/30\n45/45 [==============================] - ETA: 0s - loss: 0.6080 - categorical_accuracy: 0.7888\nEpoch 7: saving model to model_init_2024-03-0315_29_16.612602/model-00007-0.60800-0.78884-7.66915-0.30000.h5\n45/45 [==============================] - 73s 2s/step - loss: 0.6080 - categorical_accuracy: 0.7888 - val_loss: 7.6691 - val_categorical_accuracy: 0.3000 - lr: 0.0010\nEpoch 8/30\n45/45 [==============================] - ETA: 0s - loss: 0.6045 - categorical_accuracy: 0.7798\nEpoch 8: saving model to model_init_2024-03-0315_29_16.612602/model-00008-0.60450-0.77979-8.06716-0.31000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.6045 - categorical_accuracy: 0.7798 - val_loss: 8.0672 - val_categorical_accuracy: 0.3100 - lr: 0.0010\nEpoch 9/30\n45/45 [==============================] - ETA: 0s - loss: 0.5257 - categorical_accuracy: 0.7949\nEpoch 9: saving model to model_init_2024-03-0315_29_16.612602/model-00009-0.52573-0.79487-5.38186-0.32000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.5257 - categorical_accuracy: 0.7949 - val_loss: 5.3819 - val_categorical_accuracy: 0.3200 - lr: 0.0010\nEpoch 10/30\n45/45 [==============================] - ETA: 0s - loss: 0.6150 - categorical_accuracy: 0.7511\nEpoch 10: saving model to model_init_2024-03-0315_29_16.612602/model-00010-0.61499-0.75113-3.45167-0.31000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.6150 - categorical_accuracy: 0.7511 - val_loss: 3.4517 - val_categorical_accuracy: 0.3100 - lr: 0.0010\nEpoch 11/30\n45/45 [==============================] - ETA: 0s - loss: 0.4702 - categorical_accuracy: 0.8190\nEpoch 11: saving model to model_init_2024-03-0315_29_16.612602/model-00011-0.47024-0.81900-2.56021-0.30000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.4702 - categorical_accuracy: 0.8190 - val_loss: 2.5602 - val_categorical_accuracy: 0.3000 - lr: 0.0010\nEpoch 12/30\n45/45 [==============================] - ETA: 0s - loss: 0.3700 - categorical_accuracy: 0.8431\nEpoch 12: saving model to model_init_2024-03-0315_29_16.612602/model-00012-0.36999-0.84314-3.56849-0.40000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.3700 - categorical_accuracy: 0.8431 - val_loss: 3.5685 - val_categorical_accuracy: 0.4000 - lr: 0.0010\nEpoch 13/30\n45/45 [==============================] - ETA: 0s - loss: 0.3666 - categorical_accuracy: 0.8733\nEpoch 13: saving model to model_init_2024-03-0315_29_16.612602/model-00013-0.36664-0.87330-3.73233-0.36000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.3666 - categorical_accuracy: 0.8733 - val_loss: 3.7323 - val_categorical_accuracy: 0.3600 - lr: 0.0010\nEpoch 14/30\n45/45 [==============================] - ETA: 0s - loss: 0.3802 - categorical_accuracy: 0.8748\nEpoch 14: saving model to model_init_2024-03-0315_29_16.612602/model-00014-0.38018-0.87481-1.32997-0.60000.h5\n45/45 [==============================] - 75s 2s/step - loss: 0.3802 - categorical_accuracy: 0.8748 - val_loss: 1.3300 - val_categorical_accuracy: 0.6000 - lr: 0.0010\nEpoch 15/30\n45/45 [==============================] - ETA: 0s - loss: 0.2904 - categorical_accuracy: 0.8989\nEpoch 15: saving model to model_init_2024-03-0315_29_16.612602/model-00015-0.29043-0.89894-1.61801-0.55000.h5\n45/45 [==============================] - 82s 2s/step - loss: 0.2904 - categorical_accuracy: 0.8989 - val_loss: 1.6180 - val_categorical_accuracy: 0.5500 - lr: 0.0010\nEpoch 16/30\n45/45 [==============================] - ETA: 0s - loss: 0.3079 - categorical_accuracy: 0.8839\nEpoch 16: saving model to model_init_2024-03-0315_29_16.612602/model-00016-0.30795-0.88386-1.49565-0.64000.h5\n45/45 [==============================] - 79s 2s/step - loss: 0.3079 - categorical_accuracy: 0.8839 - val_loss: 1.4956 - val_categorical_accuracy: 0.6400 - lr: 0.0010\nEpoch 17/30\n45/45 [==============================] - ETA: 0s - loss: 0.3993 - categorical_accuracy: 0.8582\nEpoch 17: saving model to model_init_2024-03-0315_29_16.612602/model-00017-0.39926-0.85822-2.18048-0.51000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.3993 - categorical_accuracy: 0.8582 - val_loss: 2.1805 - val_categorical_accuracy: 0.5100 - lr: 0.0010\nEpoch 18/30\n45/45 [==============================] - ETA: 0s - loss: 0.2736 - categorical_accuracy: 0.9020\nEpoch 18: saving model to model_init_2024-03-0315_29_16.612602/model-00018-0.27361-0.90196-1.57163-0.62000.h5\n45/45 [==============================] - 72s 2s/step - loss: 0.2736 - categorical_accuracy: 0.9020 - val_loss: 1.5716 - val_categorical_accuracy: 0.6200 - lr: 0.0010\nEpoch 19/30\n45/45 [==============================] - ETA: 0s - loss: 0.2063 - categorical_accuracy: 0.9246\nEpoch 19: saving model to model_init_2024-03-0315_29_16.612602/model-00019-0.20633-0.92459-1.63832-0.62000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.2063 - categorical_accuracy: 0.9246 - val_loss: 1.6383 - val_categorical_accuracy: 0.6200 - lr: 0.0010\nEpoch 20/30\n45/45 [==============================] - ETA: 0s - loss: 0.2017 - categorical_accuracy: 0.9276\nEpoch 20: saving model to model_init_2024-03-0315_29_16.612602/model-00020-0.20169-0.92760-1.09318-0.65000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.2017 - categorical_accuracy: 0.9276 - val_loss: 1.0932 - val_categorical_accuracy: 0.6500 - lr: 0.0010\nEpoch 21/30\n45/45 [==============================] - ETA: 0s - loss: 0.4429 - categorical_accuracy: 0.8597\nEpoch 21: saving model to model_init_2024-03-0315_29_16.612602/model-00021-0.44287-0.85973-2.90883-0.41000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.4429 - categorical_accuracy: 0.8597 - val_loss: 2.9088 - val_categorical_accuracy: 0.4100 - lr: 0.0010\nEpoch 22/30\n45/45 [==============================] - ETA: 0s - loss: 0.3324 - categorical_accuracy: 0.8929\nEpoch 22: saving model to model_init_2024-03-0315_29_16.612602/model-00022-0.33244-0.89291-1.48433-0.50000.h5\n45/45 [==============================] - 79s 2s/step - loss: 0.3324 - categorical_accuracy: 0.8929 - val_loss: 1.4843 - val_categorical_accuracy: 0.5000 - lr: 0.0010\nEpoch 23/30\n45/45 [==============================] - ETA: 0s - loss: 0.3622 - categorical_accuracy: 0.8522\nEpoch 23: saving model to model_init_2024-03-0315_29_16.612602/model-00023-0.36221-0.85219-1.54422-0.59000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.3622 - categorical_accuracy: 0.8522 - val_loss: 1.5442 - val_categorical_accuracy: 0.5900 - lr: 0.0010\nEpoch 24/30\n45/45 [==============================] - ETA: 0s - loss: 0.2240 - categorical_accuracy: 0.9321\nEpoch 24: saving model to model_init_2024-03-0315_29_16.612602/model-00024-0.22400-0.93213-0.93862-0.66000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.2240 - categorical_accuracy: 0.9321 - val_loss: 0.9386 - val_categorical_accuracy: 0.6600 - lr: 0.0010\nEpoch 25/30\n45/45 [==============================] - ETA: 0s - loss: 0.3144 - categorical_accuracy: 0.8944\nEpoch 25: saving model to model_init_2024-03-0315_29_16.612602/model-00025-0.31442-0.89442-0.66863-0.73000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.3144 - categorical_accuracy: 0.8944 - val_loss: 0.6686 - val_categorical_accuracy: 0.7300 - lr: 0.0010\nEpoch 26/30\n45/45 [==============================] - ETA: 0s - loss: 0.3299 - categorical_accuracy: 0.8869\nEpoch 26: saving model to model_init_2024-03-0315_29_16.612602/model-00026-0.32987-0.88688-1.06794-0.68000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.3299 - categorical_accuracy: 0.8869 - val_loss: 1.0679 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 27/30\n45/45 [==============================] - ETA: 0s - loss: 0.3082 - categorical_accuracy: 0.8959\nEpoch 27: saving model to model_init_2024-03-0315_29_16.612602/model-00027-0.30821-0.89593-1.30123-0.62000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.3082 - categorical_accuracy: 0.8959 - val_loss: 1.3012 - val_categorical_accuracy: 0.6200 - lr: 0.0010\nEpoch 28/30\n45/45 [==============================] - ETA: 0s - loss: 0.2952 - categorical_accuracy: 0.9020\nEpoch 28: saving model to model_init_2024-03-0315_29_16.612602/model-00028-0.29516-0.90196-2.16103-0.57000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.2952 - categorical_accuracy: 0.9020 - val_loss: 2.1610 - val_categorical_accuracy: 0.5700 - lr: 0.0010\nEpoch 29/30\n45/45 [==============================] - ETA: 0s - loss: 0.4707 - categorical_accuracy: 0.8265\nEpoch 29: saving model to model_init_2024-03-0315_29_16.612602/model-00029-0.47074-0.82655-2.83704-0.36000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.4707 - categorical_accuracy: 0.8265 - val_loss: 2.8370 - val_categorical_accuracy: 0.3600 - lr: 0.0010\nEpoch 30/30\n45/45 [==============================] - ETA: 0s - loss: 0.4537 - categorical_accuracy: 0.8341\nEpoch 30: saving model to model_init_2024-03-0315_29_16.612602/model-00030-0.45372-0.83409-1.29258-0.53000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.4537 - categorical_accuracy: 0.8341 - val_loss: 1.2926 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n<keras.src.callbacks.History at 0x7f9aac1151e0>/Project_data/Project_data/train ; batch size = 15 :1: UserWarning: Model.fit_generator is deprecated and will be removed in a future version. Please use Model.fit, which supports generators. model_3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0) :13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use import imageio.v2 as imageio or call imageio.v2.imread directly. image = imread(source_path+'/'+ t[folder + (batchbatch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32) Epoch 1/30 44/45 [============================>.] - ETA: 1s - loss: 1.4500 - categorical_accuracy: 0.4727:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use import imageio.v2 as imageio or call imageio.v2.imread directly. image = imread(source_path+'/'+ t[folder + (num_batchesbatch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32) 45/45 [==============================] - ETA: 0s - loss: 1.4532 - categorical_accuracy: 0.4721Source path = /content/drive/MyDrive/Colab Notebooks/Project_data/Project_data/val ; batch size = 15\n\nEpoch 1: saving model to model_init_2024-03-0312_35_43.575611/model-00001-1.45318-0.47210-7.98422-0.16000.h5 45/45 [==============================] - 118s 2s/step - loss: 1.4532 - categorical_accuracy: 0.4721 - val_loss: 7.9842 - val_categorical_accuracy: 0.1600 - lr: 0.0010 Epoch 2/30 /usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via model.save(). This file format is considered legacy. We recommend using instead the native Keras format, e.g. model.save('my_model.keras'). saving_api.save_model( 45/45 [==============================] - ETA: 0s - loss: 0.9139 - categorical_accuracy: 0.6486 \nEpoch 2: saving model to model_init_2024-03-0312_35_43.575611/model-00002-0.91391-0.64857-16.36342-0.13000.h5 45/45 [==============================] - 99s 2s/step - loss: 0.9139 - categorical_accuracy: 0.6486 - val_loss: 16.3634 - val_categorical_accuracy: 0.1300 - lr: 0.0010 Epoch 3/30 45/45 [==============================] - ETA: 0s - loss: 0.8474 - categorical_accuracy: 0.6456 \nEpoch 3: saving model to model_init_2024-03-0312_35_43.575611/model-00003-0.84739-0.64555-16.90599-0.18000.h5 45/45 [==============================] - 98s 2s/step - loss: 0.8474 - categorical_accuracy: 0.6456 - val_loss: 16.9060 - val_categorical_accuracy: 0.1800 - lr: 0.0010 Epoch 4/30 45/45 [==============================] - ETA: 0s - loss: 0.7686 - categorical_accuracy: 0.7164 \nEpoch 4: saving model to model_init_2024-03-0312_35_43.575611/model-00004-0.76862-0.71644-21.85694-0.17000.h5 45/45 [==============================] - 102s 2s/step - loss: 0.7686 - categorical_accuracy: 0.7164 - val_loss: 21.8569 - val_categorical_accuracy: 0.1700 - lr: 0.0010 Epoch 5/30 45/45 [==============================] - ETA: 0s - loss: 0.5994 - categorical_accuracy: 0.7979 \nEpoch 5: saving model to model_init_2024-03-0312_35_43.575611/model-00005-0.59942-0.79789-25.26162-0.17000.h5 45/45 [==============================] - 101s 2s/step - loss: 0.5994 - categorical_accuracy: 0.7979 - val_loss: 25.2616 - val_categorical_accuracy: 0.1700 - lr: 0.0010 Epoch 6/30 45/45 [==============================] - ETA: 0s - loss: 0.6081 - categorical_accuracy: 0.7934 \nEpoch 6: saving model to model_init_2024-03-0312_35_43.575611/model-00006-0.60806-0.79336-14.68379-0.14000.h5 45/45 [==============================] - 100s 2s/step - loss: 0.6081 - categorical_accuracy: 0.7934 - val_loss: 14.6838 - val_categorical_accuracy: 0.1400 - lr: 0.0010 Epoch 7/30 45/45 [==============================] - ETA: 0s - loss: 0.4069 - categorical_accuracy: 0.8582 \nEpoch 7: saving model to model_init_2024-03-0312_35_43.575611/model-00007-0.40690-0.85822-10.88318-0.31000.h5 45/45 [==============================] - 101s 2s/step - loss: 0.4069 - categorical_accuracy: 0.8582 - val_loss: 10.8832 - val_categorical_accuracy: 0.3100 - lr: 0.0010 Epoch 8/30 45/45 [==============================] - ETA: 0s - loss: 0.4059 - categorical_accuracy: 0.8643 \nEpoch 8: saving model to model_init_2024-03-0312_35_43.575611/model-00008-0.40593-0.86425-9.67690-0.23000.h5 45/45 [==============================] - 98s 2s/step - loss: 0.4059 - categorical_accuracy: 0.8643 - val_loss: 9.6769 - val_categorical_accuracy: 0.2300 - lr: 0.0010 Epoch 9/30 45/45 [==============================] - ETA: 0s - loss: 0.4068 - categorical_accuracy: 0.8492 \nEpoch 9: saving model to model_init_2024-03-0312_35_43.575611/model-00009-0.40679-0.84917-5.77921-0.20000.h5 45/45 [==============================] - 102s 2s/step - loss: 0.4068 - categorical_accuracy: 0.8492 - val_loss: 5.7792 - val_categorical_accuracy: 0.2000 - lr: 0.0010 Epoch 10/30 45/45 [==============================] - ETA: 0s - loss: 0.4271 - categorical_accuracy: 0.8552 \nEpoch 10: saving model to model_init_2024-03-0312_35_43.575611/model-00010-0.42709-0.85520-2.95347-0.25000.h5 45/45 [==============================] - 103s 2s/step - loss: 0.4271 - categorical_accuracy: 0.8552 - val_loss: 2.9535 - val_categorical_accuracy: 0.2500 - lr: 0.0010 Epoch 11/30 45/45 [==============================] - ETA: 0s - loss: 0.3398 - categorical_accuracy: 0.8643 \nEpoch 11: saving model to model_init_2024-03-0312_35_43.575611/model-00011-0.33984-0.86425-4.72249-0.19000.h5 45/45 [==============================] - 102s 2s/step - loss: 0.3398 - categorical_accuracy: 0.8643 - val_loss: 4.7225 - val_categorical_accuracy: 0.1900 - lr: 0.0010 Epoch 12/30 45/45 [==============================] - ETA: 0s - loss: 0.3317 - categorical_accuracy: 0.8688 \nEpoch 12: saving model to model_init_2024-03-0312_35_43.575611/model-00012-0.33172-0.86878-2.51660-0.38000.h5 45/45 [==============================] - 101s 2s/step - loss: 0.3317 - categorical_accuracy: 0.8688 - val_loss: 2.5166 - val_categorical_accuracy: 0.3800 - lr: 0.0010 Epoch 13/30 45/45 [==============================] - ETA: 0s - loss: 0.2177 - categorical_accuracy: 0.9351 \nEpoch 13: saving model to model_init_2024-03-0312_35_43.575611/model-00013-0.21771-0.93514-0.92246-0.70000.h5 45/45 [==============================] - 99s 2s/step - loss: 0.2177 - categorical_accuracy: 0.9351 - val_loss: 0.9225 - val_categorical_accuracy: 0.7000 - lr: 0.0010 Epoch 14/30 45/45 [==============================] - ETA: 0s - loss: 0.1922 - categorical_accuracy: 0.9412 \nEpoch 14: saving model to model_init_2024-03-0312_35_43.575611/model-00014-0.19220-0.94118-1.27620-0.50000.h5 45/45 [==============================] - 103s 2s/step - loss: 0.1922 - categorical_accuracy: 0.9412 - val_loss: 1.2762 - val_categorical_accuracy: 0.5000 - lr: 0.0010 Epoch 15/30 45/45 [==============================] - ETA: 0s - loss: 0.2002 - categorical_accuracy: 0.9336 \nEpoch 15: saving model to model_init_2024-03-0312_35_43.575611/model-00015-0.20021-0.93363-1.25185-0.56000.h5 45/45 [==============================] - 97s 2s/step - loss: 0.2002 - categorical_accuracy: 0.9336 - val_loss: 1.2519 - val_categorical_accuracy: 0.5600 - lr: 0.0010 Epoch 16/30 45/45 [==============================] - ETA: 0s - loss: 0.4257 - categorical_accuracy: 0.8386 \nEpoch 16: saving model to model_init_2024-03-0312_35_43.575611/model-00016-0.42567-0.83861-20.58014-0.22000.h5 45/45 [==============================] - 95s 2s/step - loss: 0.4257 - categorical_accuracy: 0.8386 - val_loss: 20.5801 - val_categorical_accuracy: 0.2200 - lr: 0.0010 Epoch 17/30 45/45 [==============================] - ETA: 0s - loss: 0.4324 - categorical_accuracy: 0.8356 \nEpoch 17: saving model to model_init_2024-03-0312_35_43.575611/model-00017-0.43239-0.83560-4.51925-0.55000.h5 45/45 [==============================] - 96s 2s/step - loss: 0.4324 - categorical_accuracy: 0.8356 - val_loss: 4.5192 - val_categorical_accuracy: 0.5500 - lr: 0.0010 Epoch 18/30 45/45 [==============================] - ETA: 0s - loss: 0.3062 - categorical_accuracy: 0.8884 \nEpoch 18: saving model to model_init_2024-03-0312_35_43.575611/model-00018-0.30618-0.88839-0.86285-0.72000.h5 45/45 [==============================] - 95s 2s/step - loss: 0.3062 - categorical_accuracy: 0.8884 - val_loss: 0.8629 - val_categorical_accuracy: 0.7200 - lr: 0.0010 Epoch 19/30 45/45 [==============================] - ETA: 0s - loss: 0.2185 - categorical_accuracy: 0.9276 \nEpoch 19: saving model to model_init_2024-03-0312_35_43.575611/model-00019-0.21849-0.92760-2.18978-0.46000.h5 45/45 [==============================] - 98s 2s/step - loss: 0.2185 - categorical_accuracy: 0.9276 - val_loss: 2.1898 - val_categorical_accuracy: 0.4600 - lr: 0.0010 Epoch 20/30 45/45 [==============================] - ETA: 0s - loss: 0.2089 - categorical_accuracy: 0.9412 \nEpoch 20: saving model to model_init_2024-03-0312_35_43.575611/model-00020-0.20894-0.94118-1.77654-0.51000.h5 45/45 [==============================] - 100s 2s/step - loss: 0.2089 - categorical_accuracy: 0.9412 - val_loss: 1.7765 - val_categorical_accuracy: 0.5100 - lr: 0.0010 Epoch 21/30 45/45 [==============================] - ETA: 0s - loss: 0.2961 - categorical_accuracy: 0.8974 \nEpoch 21: saving model to model_init_2024-03-0312_35_43.575611/model-00021-0.29606-0.89744-0.49252-0.83000.h5 45/45 [==============================] - 97s 2s/step - loss: 0.2961 - categorical_accuracy: 0.8974 - val_loss: 0.4925 - val_categorical_accuracy: 0.8300 - lr: 0.0010 Epoch 22/30 45/45 [==============================] - ETA: 0s - loss: 0.2034 - categorical_accuracy: 0.9291 \nEpoch 22: saving model to model_init_2024-03-0312_35_43.575611/model-00022-0.20338-0.92911-0.79607-0.73000.h5 45/45 [==============================] - 101s 2s/step - loss: 0.2034 - categorical_accuracy: 0.9291 - val_loss: 0.7961 - val_categorical_accuracy: 0.7300 - lr: 0.0010 Epoch 23/30 45/45 [==============================] - ETA: 0s - loss: 0.1754 - categorical_accuracy: 0.9397 \nEpoch 23: saving model to model_init_2024-03-0312_35_43.575611/model-00023-0.17538-0.93967-1.16336-0.74000.h5 45/45 [==============================] - 99s 2s/step - loss: 0.1754 - categorical_accuracy: 0.9397 - val_loss: 1.1634 - val_categorical_accuracy: 0.7400 - lr: 0.0010 Epoch 24/30 45/45 [==============================] - ETA: 0s - loss: 0.2523 - categorical_accuracy: 0.9050 \nEpoch 24: saving model to model_init_2024-03-0312_35_43.575611/model-00024-0.25229-0.90498-1.20203-0.72000.h5 45/45 [==============================] - 99s 2s/step - loss: 0.2523 - categorical_accuracy: 0.9050 - val_loss: 1.2020 - val_categorical_accuracy: 0.7200 - lr: 0.0010 Epoch 25/30 45/45 [==============================] - ETA: 0s - loss: 0.1711 - categorical_accuracy: 0.9351 \nEpoch 25: saving model to model_init_2024-03-0312_35_43.575611/model-00025-0.17110-0.93514-0.85999-0.71000.h5 45/45 [==============================] - 97s 2s/step - loss: 0.1711 - categorical_accuracy: 0.9351 - val_loss: 0.8600 - val_categorical_accuracy: 0.7100 - lr: 0.0010 Epoch 26/30 45/45 [==============================] - ETA: 0s - loss: 0.1236 - categorical_accuracy: 0.9593 \nEpoch 26: saving model to model_init_2024-03-0312_35_43.575611/model-00026-0.12363-0.95928-0.50714-0.81000.h5 45/45 [==============================] - 100s 2s/step - loss: 0.1236 - categorical_accuracy: 0.9593 - val_loss: 0.5071 - val_categorical_accuracy: 0.8100 - lr: 0.0010 Epoch 27/30 45/45 [==============================] - ETA: 0s - loss: 0.1060 - categorical_accuracy: 0.9668 \nEpoch 27: saving model to model_init_2024-03-0312_35_43.575611/model-00027-0.10599-0.96682-0.82720-0.74000.h5 45/45 [==============================] - 97s 2s/step - loss: 0.1060 - categorical_accuracy: 0.9668 - val_loss: 0.8272 - val_categorical_accuracy: 0.7400 - lr: 0.0010 Epoch 28/30 45/45 [==============================] - ETA: 0s - loss: 0.0961 - categorical_accuracy: 0.9683 \nEpoch 28: saving model to model_init_2024-03-0312_35_43.575611/model-00028-0.09608-0.96833-0.46425-0.83000.h5 45/45 [==============================] - 100s 2s/step - loss: 0.0961 - categorical_accuracy: 0.9683 - val_loss: 0.4643 - val_categorical_accuracy: 0.8300 - lr: 0.0010 Epoch 29/30 45/45 [==============================] - ETA: 0s - loss: 0.0985 - categorical_accuracy: 0.9713 \nEpoch 29: saving model to model_init_2024-03-0312_35_43.575611/model-00029-0.09850-0.97134-0.32871-0.88000.h5 45/45 [==============================] - 98s 2s/step - loss: 0.0985 - categorical_accuracy: 0.9713 - val_loss: 0.3287 - val_categorical_accuracy: 0.8800 - lr: 0.0010 Epoch 30/30 45/45 [==============================] - ETA: 0s - loss: 0.1892 - categorical_accuracy: 0.9382 \nEpoch 30: saving model to model_init_2024-03-0312_35_43.575611/model-00030-0.18919-0.93816-1.53691-0.70000.h5 45/45 [==============================] - 102s 2s/step - loss: 0.1892 - categorical_accuracy: 0.9382 - val_loss: 1.5369 - val_categorical_accuracy: 0.7000 - lr: 0.0010 <keras.src.callbacks.History at 0x7fa700406e30>","metadata":{}},{"cell_type":"markdown","source":"The best validation accuracy is 88% and training accuracy of 97%. The difference is lesser now compared to the previous models. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"VrFRaNe3Xwmp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 4 - Batch 15, epochs 30, dropout 0.4","metadata":{}},{"cell_type":"code","source":"batch_size = 15\nnum_epochs = 30\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:58:15.105329Z","iopub.execute_input":"2024-03-06T07:58:15.105697Z","iopub.status.idle":"2024-03-06T07:58:15.110469Z","shell.execute_reply.started":"2024-03-06T07:58:15.105668Z","shell.execute_reply":"2024-03-06T07:58:15.109487Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:58:18.259896Z","iopub.execute_input":"2024-03-06T07:58:18.260278Z","iopub.status.idle":"2024-03-06T07:58:18.265754Z","shell.execute_reply.started":"2024-03-06T07:58:18.260250Z","shell.execute_reply":"2024-03-06T07:58:18.264789Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model_4 = generate_model(batch_size, img_height, img_width, 0.4, 128)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:58:21.239808Z","iopub.execute_input":"2024-03-06T07:58:21.240508Z","iopub.status.idle":"2024-03-06T07:58:21.460684Z","shell.execute_reply.started":"2024-03-06T07:58:21.240477Z","shell.execute_reply":"2024-03-06T07:58:21.459945Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"optimiser = 'sgd' #write your optimizer\nmodel_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_4.summary())","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:58:29.311653Z","iopub.execute_input":"2024-03-06T07:58:29.311997Z","iopub.status.idle":"2024-03-06T07:58:29.384615Z","shell.execute_reply.started":"2024-03-06T07:58:29.311969Z","shell.execute_reply":"2024-03-06T07:58:29.383753Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv3d_12 (Conv3D)          (None, 15, 120, 120, 16   1312      \n                             )                                   \n                                                                 \n activation_12 (Activation)  (None, 15, 120, 120, 16   0         \n                             )                                   \n                                                                 \n batch_normalization_18 (Ba  (None, 15, 120, 120, 16   64        \n tchNormalization)           )                                   \n                                                                 \n max_pooling3d_12 (MaxPooli  (None, 7, 60, 60, 16)     0         \n ng3D)                                                           \n                                                                 \n conv3d_13 (Conv3D)          (None, 7, 60, 60, 32)     4128      \n                                                                 \n activation_13 (Activation)  (None, 7, 60, 60, 32)     0         \n                                                                 \n batch_normalization_19 (Ba  (None, 7, 60, 60, 32)     128       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_13 (MaxPooli  (None, 3, 30, 30, 32)     0         \n ng3D)                                                           \n                                                                 \n conv3d_14 (Conv3D)          (None, 3, 30, 30, 64)     16448     \n                                                                 \n activation_14 (Activation)  (None, 3, 30, 30, 64)     0         \n                                                                 \n batch_normalization_20 (Ba  (None, 3, 30, 30, 64)     256       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_14 (MaxPooli  (None, 1, 15, 15, 64)     0         \n ng3D)                                                           \n                                                                 \n conv3d_15 (Conv3D)          (None, 1, 15, 15, 128)    65664     \n                                                                 \n activation_15 (Activation)  (None, 1, 15, 15, 128)    0         \n                                                                 \n batch_normalization_21 (Ba  (None, 1, 15, 15, 128)    512       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_15 (MaxPooli  (None, 1, 8, 8, 128)      0         \n ng3D)                                                           \n                                                                 \n flatten_3 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_9 (Dense)             (None, 128)               1048704   \n                                                                 \n batch_normalization_22 (Ba  (None, 128)               512       \n tchNormalization)                                               \n                                                                 \n dropout_6 (Dropout)         (None, 128)               0         \n                                                                 \n dense_10 (Dense)            (None, 128)               16512     \n                                                                 \n batch_normalization_23 (Ba  (None, 128)               512       \n tchNormalization)                                               \n                                                                 \n dropout_7 (Dropout)         (None, 128)               0         \n                                                                 \n dense_11 (Dense)            (None, 5)                 645       \n                                                                 \n=================================================================\nTotal params: 1155397 (4.41 MB)\nTrainable params: 1154405 (4.40 MB)\nNon-trainable params: 992 (3.88 KB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"model_4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:58:35.660120Z","iopub.execute_input":"2024-03-06T07:58:35.660984Z","iopub.status.idle":"2024-03-06T08:34:51.973848Z","shell.execute_reply.started":"2024-03-06T07:58:35.660952Z","shell.execute_reply":"2024-03-06T08:34:51.972912Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3008718817.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  model_4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n/tmp/ipykernel_34/515647629.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"Source path =  /kaggle/input/gesture-reconition/Project_data/train ; batch size = 15\nEpoch 1/30\n44/45 [============================>.] - ETA: 2s - loss: 1.9614 - categorical_accuracy: 0.3591","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"45/45 [==============================] - ETA: 0s - loss: 1.9567 - categorical_accuracy: 0.3590Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 15\n\nEpoch 1: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00001-1.95670-0.35897-2.32275-0.21000.h5\n45/45 [==============================] - 153s 3s/step - loss: 1.9567 - categorical_accuracy: 0.3590 - val_loss: 2.3228 - val_categorical_accuracy: 0.2100 - lr: 0.0100\nEpoch 2/30\n45/45 [==============================] - ETA: 0s - loss: 1.4882 - categorical_accuracy: 0.4434\nEpoch 2: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00002-1.48820-0.44344-5.53153-0.19000.h5\n45/45 [==============================] - 102s 2s/step - loss: 1.4882 - categorical_accuracy: 0.4434 - val_loss: 5.5315 - val_categorical_accuracy: 0.1900 - lr: 0.0100\nEpoch 3/30\n45/45 [==============================] - ETA: 0s - loss: 1.3266 - categorical_accuracy: 0.5128\nEpoch 3: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00003-1.32656-0.51282-5.76078-0.17000.h5\n45/45 [==============================] - 70s 2s/step - loss: 1.3266 - categorical_accuracy: 0.5128 - val_loss: 5.7608 - val_categorical_accuracy: 0.1700 - lr: 0.0100\nEpoch 4/30\n45/45 [==============================] - ETA: 0s - loss: 1.1984 - categorical_accuracy: 0.5324\nEpoch 4: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00004-1.19840-0.53243-6.95512-0.19000.h5\n45/45 [==============================] - 71s 2s/step - loss: 1.1984 - categorical_accuracy: 0.5324 - val_loss: 6.9551 - val_categorical_accuracy: 0.1900 - lr: 0.0100\nEpoch 5/30\n45/45 [==============================] - ETA: 0s - loss: 1.0991 - categorical_accuracy: 0.5822\nEpoch 5: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00005-1.09913-0.58220-11.90950-0.16000.h5\n45/45 [==============================] - 60s 1s/step - loss: 1.0991 - categorical_accuracy: 0.5822 - val_loss: 11.9095 - val_categorical_accuracy: 0.1600 - lr: 0.0100\nEpoch 6/30\n45/45 [==============================] - ETA: 0s - loss: 0.9664 - categorical_accuracy: 0.6290\nEpoch 6: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00006-0.96643-0.62896-5.06414-0.23000.h5\n45/45 [==============================] - 62s 1s/step - loss: 0.9664 - categorical_accuracy: 0.6290 - val_loss: 5.0641 - val_categorical_accuracy: 0.2300 - lr: 0.0100\nEpoch 7/30\n45/45 [==============================] - ETA: 0s - loss: 0.9296 - categorical_accuracy: 0.6275\nEpoch 7: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00007-0.92955-0.62745-4.55166-0.28000.h5\n45/45 [==============================] - 74s 2s/step - loss: 0.9296 - categorical_accuracy: 0.6275 - val_loss: 4.5517 - val_categorical_accuracy: 0.2800 - lr: 0.0020\nEpoch 8/30\n45/45 [==============================] - ETA: 0s - loss: 0.8996 - categorical_accuracy: 0.6335\nEpoch 8: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00008-0.89960-0.63348-4.53371-0.26000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.8996 - categorical_accuracy: 0.6335 - val_loss: 4.5337 - val_categorical_accuracy: 0.2600 - lr: 0.0020\nEpoch 9/30\n45/45 [==============================] - ETA: 0s - loss: 0.8398 - categorical_accuracy: 0.6591\nEpoch 9: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00009-0.83984-0.65913-2.27536-0.30000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.8398 - categorical_accuracy: 0.6591 - val_loss: 2.2754 - val_categorical_accuracy: 0.3000 - lr: 0.0020\nEpoch 10/30\n45/45 [==============================] - ETA: 0s - loss: 0.7445 - categorical_accuracy: 0.7134\nEpoch 10: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00010-0.74454-0.71342-1.63255-0.43000.h5\n45/45 [==============================] - 66s 1s/step - loss: 0.7445 - categorical_accuracy: 0.7134 - val_loss: 1.6326 - val_categorical_accuracy: 0.4300 - lr: 0.0020\nEpoch 11/30\n45/45 [==============================] - ETA: 0s - loss: 0.7461 - categorical_accuracy: 0.7119\nEpoch 11: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00011-0.74608-0.71192-1.22051-0.61000.h5\n45/45 [==============================] - 81s 2s/step - loss: 0.7461 - categorical_accuracy: 0.7119 - val_loss: 1.2205 - val_categorical_accuracy: 0.6100 - lr: 0.0020\nEpoch 12/30\n45/45 [==============================] - ETA: 0s - loss: 0.7153 - categorical_accuracy: 0.7179\nEpoch 12: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00012-0.71530-0.71795-0.71005-0.71000.h5\n45/45 [==============================] - 73s 2s/step - loss: 0.7153 - categorical_accuracy: 0.7179 - val_loss: 0.7101 - val_categorical_accuracy: 0.7100 - lr: 0.0020\nEpoch 13/30\n45/45 [==============================] - ETA: 0s - loss: 0.7271 - categorical_accuracy: 0.7029\nEpoch 13: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00013-0.72711-0.70287-0.70032-0.75000.h5\n45/45 [==============================] - 74s 2s/step - loss: 0.7271 - categorical_accuracy: 0.7029 - val_loss: 0.7003 - val_categorical_accuracy: 0.7500 - lr: 0.0020\nEpoch 14/30\n45/45 [==============================] - ETA: 0s - loss: 0.6934 - categorical_accuracy: 0.7240\nEpoch 14: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00014-0.69342-0.72398-0.72662-0.71000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.6934 - categorical_accuracy: 0.7240 - val_loss: 0.7266 - val_categorical_accuracy: 0.7100 - lr: 0.0020\nEpoch 15/30\n45/45 [==============================] - ETA: 0s - loss: 0.6364 - categorical_accuracy: 0.7617\nEpoch 15: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00015-0.63638-0.76169-0.66261-0.74000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.6364 - categorical_accuracy: 0.7617 - val_loss: 0.6626 - val_categorical_accuracy: 0.7400 - lr: 0.0020\nEpoch 16/30\n45/45 [==============================] - ETA: 0s - loss: 0.6206 - categorical_accuracy: 0.7421\nEpoch 16: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00016-0.62058-0.74208-0.72511-0.73000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.6206 - categorical_accuracy: 0.7421 - val_loss: 0.7251 - val_categorical_accuracy: 0.7300 - lr: 0.0020\nEpoch 17/30\n45/45 [==============================] - ETA: 0s - loss: 0.6044 - categorical_accuracy: 0.7677\nEpoch 17: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00017-0.60443-0.76772-0.74206-0.74000.h5\n45/45 [==============================] - 66s 1s/step - loss: 0.6044 - categorical_accuracy: 0.7677 - val_loss: 0.7421 - val_categorical_accuracy: 0.7400 - lr: 0.0020\nEpoch 18/30\n45/45 [==============================] - ETA: 0s - loss: 0.6325 - categorical_accuracy: 0.7602\nEpoch 18: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00018-0.63247-0.76018-0.63545-0.77000.h5\n45/45 [==============================] - 62s 1s/step - loss: 0.6325 - categorical_accuracy: 0.7602 - val_loss: 0.6354 - val_categorical_accuracy: 0.7700 - lr: 0.0020\nEpoch 19/30\n45/45 [==============================] - ETA: 0s - loss: 0.5829 - categorical_accuracy: 0.7828\nEpoch 19: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00019-0.58295-0.78281-0.79612-0.74000.h5\n45/45 [==============================] - 67s 2s/step - loss: 0.5829 - categorical_accuracy: 0.7828 - val_loss: 0.7961 - val_categorical_accuracy: 0.7400 - lr: 0.0020\nEpoch 20/30\n45/45 [==============================] - ETA: 0s - loss: 0.5345 - categorical_accuracy: 0.8024\nEpoch 20: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00020-0.53451-0.80241-0.51518-0.83000.h5\n45/45 [==============================] - 69s 2s/step - loss: 0.5345 - categorical_accuracy: 0.8024 - val_loss: 0.5152 - val_categorical_accuracy: 0.8300 - lr: 0.0020\nEpoch 21/30\n45/45 [==============================] - ETA: 0s - loss: 0.5336 - categorical_accuracy: 0.8054\nEpoch 21: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00021-0.53362-0.80543-0.63647-0.77000.h5\n45/45 [==============================] - 69s 2s/step - loss: 0.5336 - categorical_accuracy: 0.8054 - val_loss: 0.6365 - val_categorical_accuracy: 0.7700 - lr: 0.0020\nEpoch 22/30\n45/45 [==============================] - ETA: 0s - loss: 0.4947 - categorical_accuracy: 0.8039\nEpoch 22: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00022-0.49470-0.80392-0.83114-0.74000.h5\n45/45 [==============================] - 67s 2s/step - loss: 0.4947 - categorical_accuracy: 0.8039 - val_loss: 0.8311 - val_categorical_accuracy: 0.7400 - lr: 0.0020\nEpoch 23/30\n45/45 [==============================] - ETA: 0s - loss: 0.5503 - categorical_accuracy: 0.7979\nEpoch 23: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00023-0.55028-0.79789-1.73500-0.50000.h5\n45/45 [==============================] - 67s 2s/step - loss: 0.5503 - categorical_accuracy: 0.7979 - val_loss: 1.7350 - val_categorical_accuracy: 0.5000 - lr: 0.0020\nEpoch 24/30\n45/45 [==============================] - ETA: 0s - loss: 0.4838 - categorical_accuracy: 0.8235\nEpoch 24: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00024-0.48383-0.82353-0.71529-0.76000.h5\n45/45 [==============================] - 64s 1s/step - loss: 0.4838 - categorical_accuracy: 0.8235 - val_loss: 0.7153 - val_categorical_accuracy: 0.7600 - lr: 0.0020\nEpoch 25/30\n45/45 [==============================] - ETA: 0s - loss: 0.5213 - categorical_accuracy: 0.7979\nEpoch 25: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00025-0.52126-0.79789-0.74523-0.71000.h5\n45/45 [==============================] - 62s 1s/step - loss: 0.5213 - categorical_accuracy: 0.7979 - val_loss: 0.7452 - val_categorical_accuracy: 0.7100 - lr: 0.0020\nEpoch 26/30\n45/45 [==============================] - ETA: 0s - loss: 0.5039 - categorical_accuracy: 0.8054\nEpoch 26: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00026-0.50389-0.80543-0.49248-0.79000.h5\n45/45 [==============================] - 67s 2s/step - loss: 0.5039 - categorical_accuracy: 0.8054 - val_loss: 0.4925 - val_categorical_accuracy: 0.7900 - lr: 0.0010\nEpoch 27/30\n45/45 [==============================] - ETA: 0s - loss: 0.5054 - categorical_accuracy: 0.7994\nEpoch 27: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00027-0.50544-0.79940-0.57681-0.81000.h5\n45/45 [==============================] - 64s 1s/step - loss: 0.5054 - categorical_accuracy: 0.7994 - val_loss: 0.5768 - val_categorical_accuracy: 0.8100 - lr: 0.0010\nEpoch 28/30\n45/45 [==============================] - ETA: 0s - loss: 0.4576 - categorical_accuracy: 0.8386\nEpoch 28: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00028-0.45760-0.83861-0.65935-0.78000.h5\n45/45 [==============================] - 64s 1s/step - loss: 0.4576 - categorical_accuracy: 0.8386 - val_loss: 0.6593 - val_categorical_accuracy: 0.7800 - lr: 0.0010\nEpoch 29/30\n45/45 [==============================] - ETA: 0s - loss: 0.4031 - categorical_accuracy: 0.8462\nEpoch 29: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00029-0.40314-0.84615-0.65046-0.78000.h5\n45/45 [==============================] - 71s 2s/step - loss: 0.4031 - categorical_accuracy: 0.8462 - val_loss: 0.6505 - val_categorical_accuracy: 0.7800 - lr: 0.0010\nEpoch 30/30\n45/45 [==============================] - ETA: 0s - loss: 0.4266 - categorical_accuracy: 0.8522\nEpoch 30: saving model to /kaggle/working/model_init_2024-03-0606_59_34.127778/model-00030-0.42658-0.85219-0.58113-0.83000.h5\n45/45 [==============================] - 55s 1s/step - loss: 0.4266 - categorical_accuracy: 0.8522 - val_loss: 0.5811 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7f057ef299f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"Epoch 1/30\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1709552587.359955     105 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n44/45 [============================>.] - ETA: 2s - loss: 1.8382 - categorical_accuracy: 0.3909\n/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n45/45 [==============================] - ETA: 0s - loss: 1.8428 - categorical_accuracy: 0.3906Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 15\n\nEpoch 1: saving model to model_init_2024-03-0411_42_54.002711/model-00001-1.84276-0.39065-9.01466-0.21000.h5\n45/45 [==============================] - 154s 3s/step - loss: 1.8428 - categorical_accuracy: 0.3906 - val_loss: 9.0147 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 2/30\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n45/45 [==============================] - ETA: 0s - loss: 1.3755 - categorical_accuracy: 0.5023\nEpoch 2: saving model to model_init_2024-03-0411_42_54.002711/model-00002-1.37554-0.50226-18.28967-0.20000.h5\n45/45 [==============================] - 87s 2s/step - loss: 1.3755 - categorical_accuracy: 0.5023 - val_loss: 18.2897 - val_categorical_accuracy: 0.2000 - lr: 0.0010\nEpoch 3/30\n45/45 [==============================] - ETA: 0s - loss: 1.1374 - categorical_accuracy: 0.5792\nEpoch 3: saving model to model_init_2024-03-0411_42_54.002711/model-00003-1.13741-0.57919-24.65601-0.24000.h5\n45/45 [==============================] - 89s 2s/step - loss: 1.1374 - categorical_accuracy: 0.5792 - val_loss: 24.6560 - val_categorical_accuracy: 0.2400 - lr: 0.0010\nEpoch 4/30\n45/45 [==============================] - ETA: 0s - loss: 0.9714 - categorical_accuracy: 0.6305\nEpoch 4: saving model to model_init_2024-03-0411_42_54.002711/model-00004-0.97140-0.63047-27.72349-0.21000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.9714 - categorical_accuracy: 0.6305 - val_loss: 27.7235 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 5/30\n45/45 [==============================] - ETA: 0s - loss: 1.1433 - categorical_accuracy: 0.5777\nEpoch 5: saving model to model_init_2024-03-0411_42_54.002711/model-00005-1.14333-0.57768-16.12456-0.21000.h5\n45/45 [==============================] - 77s 2s/step - loss: 1.1433 - categorical_accuracy: 0.5777 - val_loss: 16.1246 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 6/30\n45/45 [==============================] - ETA: 0s - loss: 1.1868 - categorical_accuracy: 0.5732\nEpoch 6: saving model to model_init_2024-03-0411_42_54.002711/model-00006-1.18684-0.57315-8.31426-0.21000.h5\n45/45 [==============================] - 84s 2s/step - loss: 1.1868 - categorical_accuracy: 0.5732 - val_loss: 8.3143 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 7/30\n45/45 [==============================] - ETA: 0s - loss: 0.9198 - categorical_accuracy: 0.6727\nEpoch 7: saving model to model_init_2024-03-0411_42_54.002711/model-00007-0.91984-0.67270-8.25928-0.27000.h5\n45/45 [==============================] - 75s 2s/step - loss: 0.9198 - categorical_accuracy: 0.6727 - val_loss: 8.2593 - val_categorical_accuracy: 0.2700 - lr: 0.0010\nEpoch 8/30\n45/45 [==============================] - ETA: 0s - loss: 0.9736 - categorical_accuracy: 0.6154\nEpoch 8: saving model to model_init_2024-03-0411_42_54.002711/model-00008-0.97363-0.61538-6.37939-0.24000.h5\n45/45 [==============================] - 79s 2s/step - loss: 0.9736 - categorical_accuracy: 0.6154 - val_loss: 6.3794 - val_categorical_accuracy: 0.2400 - lr: 0.0010\nEpoch 9/30\n45/45 [==============================] - ETA: 0s - loss: 0.8369 - categorical_accuracy: 0.6591\nEpoch 9: saving model to model_init_2024-03-0411_42_54.002711/model-00009-0.83694-0.65913-6.36008-0.31000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.8369 - categorical_accuracy: 0.6591 - val_loss: 6.3601 - val_categorical_accuracy: 0.3100 - lr: 0.0010\nEpoch 10/30\n45/45 [==============================] - ETA: 0s - loss: 0.7112 - categorical_accuracy: 0.7119\nEpoch 10: saving model to model_init_2024-03-0411_42_54.002711/model-00010-0.71125-0.71192-4.15338-0.35000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.7112 - categorical_accuracy: 0.7119 - val_loss: 4.1534 - val_categorical_accuracy: 0.3500 - lr: 0.0010\nEpoch 11/30\n45/45 [==============================] - ETA: 0s - loss: 0.5966 - categorical_accuracy: 0.7707\nEpoch 11: saving model to model_init_2024-03-0411_42_54.002711/model-00011-0.59660-0.77074-2.90437-0.31000.h5\n45/45 [==============================] - 74s 2s/step - loss: 0.5966 - categorical_accuracy: 0.7707 - val_loss: 2.9044 - val_categorical_accuracy: 0.3100 - lr: 0.0010\nEpoch 12/30\n45/45 [==============================] - ETA: 0s - loss: 0.5585 - categorical_accuracy: 0.7903\nEpoch 12: saving model to model_init_2024-03-0411_42_54.002711/model-00012-0.55846-0.79035-1.91452-0.41000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.5585 - categorical_accuracy: 0.7903 - val_loss: 1.9145 - val_categorical_accuracy: 0.4100 - lr: 0.0010\nEpoch 13/30\n45/45 [==============================] - ETA: 0s - loss: 0.4934 - categorical_accuracy: 0.7979\nEpoch 13: saving model to model_init_2024-03-0411_42_54.002711/model-00013-0.49344-0.79789-2.29538-0.36000.h5\n45/45 [==============================] - 74s 2s/step - loss: 0.4934 - categorical_accuracy: 0.7979 - val_loss: 2.2954 - val_categorical_accuracy: 0.3600 - lr: 0.0010\nEpoch 14/30\n45/45 [==============================] - ETA: 0s - loss: 0.4188 - categorical_accuracy: 0.8311\nEpoch 14: saving model to model_init_2024-03-0411_42_54.002711/model-00014-0.41882-0.83107-0.71817-0.75000.h5\n45/45 [==============================] - 75s 2s/step - loss: 0.4188 - categorical_accuracy: 0.8311 - val_loss: 0.7182 - val_categorical_accuracy: 0.7500 - lr: 0.0010\nEpoch 15/30\n45/45 [==============================] - ETA: 0s - loss: 0.3732 - categorical_accuracy: 0.8643\nEpoch 15: saving model to model_init_2024-03-0411_42_54.002711/model-00015-0.37322-0.86425-0.71354-0.69000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.3732 - categorical_accuracy: 0.8643 - val_loss: 0.7135 - val_categorical_accuracy: 0.6900 - lr: 0.0010\nEpoch 16/30\n45/45 [==============================] - ETA: 0s - loss: 0.3602 - categorical_accuracy: 0.8627\nEpoch 16: saving model to model_init_2024-03-0411_42_54.002711/model-00016-0.36020-0.86275-1.79631-0.47000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.3602 - categorical_accuracy: 0.8627 - val_loss: 1.7963 - val_categorical_accuracy: 0.4700 - lr: 0.0010\nEpoch 17/30\n45/45 [==============================] - ETA: 0s - loss: 0.4061 - categorical_accuracy: 0.8492\nEpoch 17: saving model to model_init_2024-03-0411_42_54.002711/model-00017-0.40614-0.84917-0.69133-0.78000.h5\n45/45 [==============================] - 80s 2s/step - loss: 0.4061 - categorical_accuracy: 0.8492 - val_loss: 0.6913 - val_categorical_accuracy: 0.7800 - lr: 0.0010\nEpoch 18/30\n45/45 [==============================] - ETA: 0s - loss: 0.4099 - categorical_accuracy: 0.8462\nEpoch 18: saving model to model_init_2024-03-0411_42_54.002711/model-00018-0.40991-0.84615-1.56948-0.52000.h5\n45/45 [==============================] - 72s 2s/step - loss: 0.4099 - categorical_accuracy: 0.8462 - val_loss: 1.5695 - val_categorical_accuracy: 0.5200 - lr: 0.0010\nEpoch 19/30\n45/45 [==============================] - ETA: 0s - loss: 0.2472 - categorical_accuracy: 0.8974\nEpoch 19: saving model to model_init_2024-03-0411_42_54.002711/model-00019-0.24718-0.89744-0.64426-0.83000.h5\n45/45 [==============================] - 78s 2s/step - loss: 0.2472 - categorical_accuracy: 0.8974 - val_loss: 0.6443 - val_categorical_accuracy: 0.8300 - lr: 0.0010\nEpoch 20/30\n45/45 [==============================] - ETA: 0s - loss: 0.3399 - categorical_accuracy: 0.8824\nEpoch 20: saving model to model_init_2024-03-0411_42_54.002711/model-00020-0.33989-0.88235-0.80026-0.76000.h5\n45/45 [==============================] - 77s 2s/step - loss: 0.3399 - categorical_accuracy: 0.8824 - val_loss: 0.8003 - val_categorical_accuracy: 0.7600 - lr: 0.0010\nEpoch 21/30\n45/45 [==============================] - ETA: 0s - loss: 0.2576 - categorical_accuracy: 0.9095\nEpoch 21: saving model to model_init_2024-03-0411_42_54.002711/model-00021-0.25764-0.90950-1.65360-0.67000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.2576 - categorical_accuracy: 0.9095 - val_loss: 1.6536 - val_categorical_accuracy: 0.6700 - lr: 0.0010\nEpoch 22/30\n45/45 [==============================] - ETA: 0s - loss: 0.2929 - categorical_accuracy: 0.8733\nEpoch 22: saving model to model_init_2024-03-0411_42_54.002711/model-00022-0.29288-0.87330-1.22758-0.65000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.2929 - categorical_accuracy: 0.8733 - val_loss: 1.2276 - val_categorical_accuracy: 0.6500 - lr: 0.0010\nEpoch 23/30\n45/45 [==============================] - ETA: 0s - loss: 0.2759 - categorical_accuracy: 0.8959\nEpoch 23: saving model to model_init_2024-03-0411_42_54.002711/model-00023-0.27594-0.89593-0.83632-0.67000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.2759 - categorical_accuracy: 0.8959 - val_loss: 0.8363 - val_categorical_accuracy: 0.6700 - lr: 0.0010\nEpoch 24/30\n45/45 [==============================] - ETA: 0s - loss: 0.3141 - categorical_accuracy: 0.9035\nEpoch 24: saving model to model_init_2024-03-0411_42_54.002711/model-00024-0.31413-0.90347-1.50733-0.62000.h5\n45/45 [==============================] - 75s 2s/step - loss: 0.3141 - categorical_accuracy: 0.9035 - val_loss: 1.5073 - val_categorical_accuracy: 0.6200 - lr: 0.0010\nEpoch 25/30\n45/45 [==============================] - ETA: 0s - loss: 0.2627 - categorical_accuracy: 0.9065\nEpoch 25: saving model to model_init_2024-03-0411_42_54.002711/model-00025-0.26270-0.90649-0.70348-0.76000.h5\n45/45 [==============================] - 76s 2s/step - loss: 0.2627 - categorical_accuracy: 0.9065 - val_loss: 0.7035 - val_categorical_accuracy: 0.7600 - lr: 0.0010\nEpoch 26/30\n45/45 [==============================] - ETA: 0s - loss: 0.2824 - categorical_accuracy: 0.9050\nEpoch 26: saving model to model_init_2024-03-0411_42_54.002711/model-00026-0.28244-0.90498-0.94104-0.69000.h5\n45/45 [==============================] - 70s 2s/step - loss: 0.2824 - categorical_accuracy: 0.9050 - val_loss: 0.9410 - val_categorical_accuracy: 0.6900 - lr: 0.0010\nEpoch 27/30\n45/45 [==============================] - ETA: 0s - loss: 0.2600 - categorical_accuracy: 0.9050\nEpoch 27: saving model to model_init_2024-03-0411_42_54.002711/model-00027-0.25995-0.90498-0.73749-0.74000.h5\n45/45 [==============================] - 62s 1s/step - loss: 0.2600 - categorical_accuracy: 0.9050 - val_loss: 0.7375 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 28/30\n45/45 [==============================] - ETA: 0s - loss: 0.3087 - categorical_accuracy: 0.8673\nEpoch 28: saving model to model_init_2024-03-0411_42_54.002711/model-00028-0.30872-0.86727-1.63807-0.61000.h5\n45/45 [==============================] - 69s 2s/step - loss: 0.3087 - categorical_accuracy: 0.8673 - val_loss: 1.6381 - val_categorical_accuracy: 0.6100 - lr: 0.0010\nEpoch 29/30\n45/45 [==============================] - ETA: 0s - loss: 0.2492 - categorical_accuracy: 0.9201\nEpoch 29: saving model to model_init_2024-03-0411_42_54.002711/model-00029-0.24916-0.92006-1.87546-0.57000.h5\n45/45 [==============================] - 71s 2s/step - loss: 0.2492 - categorical_accuracy: 0.9201 - val_loss: 1.8755 - val_categorical_accuracy: 0.5700 - lr: 0.0010\nEpoch 30/30\n45/45 [==============================] - ETA: 0s - loss: 0.2306 - categorical_accuracy: 0.9155\nEpoch 30: saving model to model_init_2024-03-0411_42_54.002711/model-00030-0.23061-0.91554-1.90569-0.66000.h5\n45/45 [==============================] - 72s 2s/step - loss: 0.2306 - categorical_accuracy: 0.9155 - val_loss: 1.9057 - val_categorical_accuracy: 0.6600 - lr: 0.0010","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 5 - Batch size 20, epochs 30, drop out 0.4 and dense 64","metadata":{}},{"cell_type":"code","source":"batch_size = 20\nnum_epochs = 30\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:49.911505Z","iopub.execute_input":"2024-03-05T11:47:49.911790Z","iopub.status.idle":"2024-03-05T11:47:49.920328Z","shell.execute_reply.started":"2024-03-05T11:47:49.911763Z","shell.execute_reply":"2024-03-05T11:47:49.919281Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:49.921611Z","iopub.execute_input":"2024-03-05T11:47:49.921903Z","iopub.status.idle":"2024-03-05T11:47:49.928620Z","shell.execute_reply.started":"2024-03-05T11:47:49.921879Z","shell.execute_reply":"2024-03-05T11:47:49.927952Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model_5 = generate_model(batch_size, img_height, img_width, 0.4, 64)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:49.929442Z","iopub.execute_input":"2024-03-05T11:47:49.929659Z","iopub.status.idle":"2024-03-05T11:47:50.138520Z","shell.execute_reply.started":"2024-03-05T11:47:49.929640Z","shell.execute_reply":"2024-03-05T11:47:50.137551Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"optimiser = 'adam' #write your optimizer\nmodel_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_4.summary())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:50.139537Z","iopub.execute_input":"2024-03-05T11:47:50.139805Z","iopub.status.idle":"2024-03-05T11:47:50.210134Z","shell.execute_reply.started":"2024-03-05T11:47:50.139784Z","shell.execute_reply":"2024-03-05T11:47:50.209491Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv3d_12 (Conv3D)          (None, 15, 120, 120, 16   1312      \n                             )                                   \n                                                                 \n activation_12 (Activation)  (None, 15, 120, 120, 16   0         \n                             )                                   \n                                                                 \n batch_normalization_18 (Ba  (None, 15, 120, 120, 16   64        \n tchNormalization)           )                                   \n                                                                 \n max_pooling3d_12 (MaxPooli  (None, 7, 60, 60, 16)     0         \n ng3D)                                                           \n                                                                 \n conv3d_13 (Conv3D)          (None, 7, 60, 60, 32)     4128      \n                                                                 \n activation_13 (Activation)  (None, 7, 60, 60, 32)     0         \n                                                                 \n batch_normalization_19 (Ba  (None, 7, 60, 60, 32)     128       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_13 (MaxPooli  (None, 3, 30, 30, 32)     0         \n ng3D)                                                           \n                                                                 \n conv3d_14 (Conv3D)          (None, 3, 30, 30, 64)     16448     \n                                                                 \n activation_14 (Activation)  (None, 3, 30, 30, 64)     0         \n                                                                 \n batch_normalization_20 (Ba  (None, 3, 30, 30, 64)     256       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_14 (MaxPooli  (None, 1, 15, 15, 64)     0         \n ng3D)                                                           \n                                                                 \n conv3d_15 (Conv3D)          (None, 1, 15, 15, 128)    65664     \n                                                                 \n activation_15 (Activation)  (None, 1, 15, 15, 128)    0         \n                                                                 \n batch_normalization_21 (Ba  (None, 1, 15, 15, 128)    512       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_15 (MaxPooli  (None, 1, 8, 8, 128)      0         \n ng3D)                                                           \n                                                                 \n flatten_3 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_9 (Dense)             (None, 128)               1048704   \n                                                                 \n batch_normalization_22 (Ba  (None, 128)               512       \n tchNormalization)                                               \n                                                                 \n dropout_6 (Dropout)         (None, 128)               0         \n                                                                 \n dense_10 (Dense)            (None, 128)               16512     \n                                                                 \n batch_normalization_23 (Ba  (None, 128)               512       \n tchNormalization)                                               \n                                                                 \n dropout_7 (Dropout)         (None, 128)               0         \n                                                                 \n dense_11 (Dense)            (None, 5)                 645       \n                                                                 \n=================================================================\nTotal params: 1155397 (4.41 MB)\nTrainable params: 1154405 (4.40 MB)\nNon-trainable params: 992 (3.88 KB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"#model_5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:50.210851Z","iopub.execute_input":"2024-03-05T11:47:50.211115Z","iopub.status.idle":"2024-03-05T11:47:50.215475Z","shell.execute_reply.started":"2024-03-05T11:47:50.211095Z","shell.execute_reply":"2024-03-05T11:47:50.214454Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Epoch 1/30\n33/34 [============================>.] - ETA: 2s - loss: 1.8639 - categorical_accuracy: 0.3530\n/tmp/ipykernel_34/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n34/34 [==============================] - ETA: 0s - loss: 1.8634 - categorical_accuracy: 0.3544Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 20\n\nEpoch 1: saving model to model_init_2024-03-0411_42_54.002711/model-00001-1.86336-0.35445-2.24560-0.16000.h5\n34/34 [==============================] - 86s 2s/step - loss: 1.8634 - categorical_accuracy: 0.3544 - val_loss: 2.2456 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 2/30\n34/34 [==============================] - ETA: 0s - loss: 1.3272 - categorical_accuracy: 0.5053\nEpoch 2: saving model to model_init_2024-03-0411_42_54.002711/model-00002-1.32720-0.50528-3.23536-0.13000.h5\n34/34 [==============================] - 78s 2s/step - loss: 1.3272 - categorical_accuracy: 0.5053 - val_loss: 3.2354 - val_categorical_accuracy: 0.1300 - lr: 0.0010\nEpoch 3/30\n34/34 [==============================] - ETA: 0s - loss: 1.2342 - categorical_accuracy: 0.5204\nEpoch 3: saving model to model_init_2024-03-0411_42_54.002711/model-00003-1.23419-0.52036-3.57203-0.16000.h5\n34/34 [==============================] - 74s 2s/step - loss: 1.2342 - categorical_accuracy: 0.5204 - val_loss: 3.5720 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 4/30\n34/34 [==============================] - ETA: 0s - loss: 1.0491 - categorical_accuracy: 0.5822\nEpoch 4: saving model to model_init_2024-03-0411_42_54.002711/model-00004-1.04908-0.58220-4.62160-0.20000.h5\n34/34 [==============================] - 69s 2s/step - loss: 1.0491 - categorical_accuracy: 0.5822 - val_loss: 4.6216 - val_categorical_accuracy: 0.2000 - lr: 0.0010\nEpoch 5/30\n34/34 [==============================] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.6365\nEpoch 5: saving model to model_init_2024-03-0411_42_54.002711/model-00005-1.00768-0.63650-6.09362-0.22000.h5\n34/34 [==============================] - 71s 2s/step - loss: 1.0077 - categorical_accuracy: 0.6365 - val_loss: 6.0936 - val_categorical_accuracy: 0.2200 - lr: 0.0010\nEpoch 6/30\n34/34 [==============================] - ETA: 0s - loss: 0.9764 - categorical_accuracy: 0.6259\nEpoch 6: saving model to model_init_2024-03-0411_42_54.002711/model-00006-0.97641-0.62594-8.90670-0.21000.h5\n34/34 [==============================] - 72s 2s/step - loss: 0.9764 - categorical_accuracy: 0.6259 - val_loss: 8.9067 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 7/30\n34/34 [==============================] - ETA: 0s - loss: 0.8302 - categorical_accuracy: 0.6938\nEpoch 7: saving model to model_init_2024-03-0411_42_54.002711/model-00007-0.83020-0.69382-7.77892-0.19000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.8302 - categorical_accuracy: 0.6938 - val_loss: 7.7789 - val_categorical_accuracy: 0.1900 - lr: 0.0010\nEpoch 8/30\n34/34 [==============================] - ETA: 0s - loss: 0.7492 - categorical_accuracy: 0.7059\nEpoch 8: saving model to model_init_2024-03-0411_42_54.002711/model-00008-0.74916-0.70588-5.89156-0.16000.h5\n34/34 [==============================] - 74s 2s/step - loss: 0.7492 - categorical_accuracy: 0.7059 - val_loss: 5.8916 - val_categorical_accuracy: 0.1600 - lr: 0.0010\nEpoch 9/30\n34/34 [==============================] - ETA: 0s - loss: 0.6672 - categorical_accuracy: 0.7526\nEpoch 9: saving model to model_init_2024-03-0411_42_54.002711/model-00009-0.66724-0.75264-5.42276-0.20000.h5\n34/34 [==============================] - 66s 2s/step - loss: 0.6672 - categorical_accuracy: 0.7526 - val_loss: 5.4228 - val_categorical_accuracy: 0.2000 - lr: 0.0010\nEpoch 10/30\n34/34 [==============================] - ETA: 0s - loss: 0.6035 - categorical_accuracy: 0.7692\nEpoch 10: saving model to model_init_2024-03-0411_42_54.002711/model-00010-0.60347-0.76923-5.15601-0.22000.h5\n34/34 [==============================] - 71s 2s/step - loss: 0.6035 - categorical_accuracy: 0.7692 - val_loss: 5.1560 - val_categorical_accuracy: 0.2200 - lr: 0.0010\nEpoch 11/30\n34/34 [==============================] - ETA: 0s - loss: 0.4734 - categorical_accuracy: 0.8220\nEpoch 11: saving model to model_init_2024-03-0411_42_54.002711/model-00011-0.47338-0.82202-4.79484-0.34000.h5\n34/34 [==============================] - 75s 2s/step - loss: 0.4734 - categorical_accuracy: 0.8220 - val_loss: 4.7948 - val_categorical_accuracy: 0.3400 - lr: 0.0010\nEpoch 12/30\n34/34 [==============================] - ETA: 0s - loss: 0.4311 - categorical_accuracy: 0.8431\nEpoch 12: saving model to model_init_2024-03-0411_42_54.002711/model-00012-0.43114-0.84314-4.13371-0.31000.h5\n34/34 [==============================] - 64s 2s/step - loss: 0.4311 - categorical_accuracy: 0.8431 - val_loss: 4.1337 - val_categorical_accuracy: 0.3100 - lr: 0.0010\nEpoch 13/30\n34/34 [==============================] - ETA: 0s - loss: 0.4908 - categorical_accuracy: 0.8115\nEpoch 13: saving model to model_init_2024-03-0411_42_54.002711/model-00013-0.49082-0.81146-4.45952-0.24000.h5\n34/34 [==============================] - 71s 2s/step - loss: 0.4908 - categorical_accuracy: 0.8115 - val_loss: 4.4595 - val_categorical_accuracy: 0.2400 - lr: 0.0010\nEpoch 14/30\n34/34 [==============================] - ETA: 0s - loss: 0.6516 - categorical_accuracy: 0.7541\nEpoch 14: saving model to model_init_2024-03-0411_42_54.002711/model-00014-0.65159-0.75415-4.14772-0.22000.h5\n34/34 [==============================] - 71s 2s/step - loss: 0.6516 - categorical_accuracy: 0.7541 - val_loss: 4.1477 - val_categorical_accuracy: 0.2200 - lr: 0.0010\nEpoch 15/30\n34/34 [==============================] - ETA: 0s - loss: 0.7039 - categorical_accuracy: 0.7360\nEpoch 15: saving model to model_init_2024-03-0411_42_54.002711/model-00015-0.70392-0.73605-2.20259-0.41000.h5\n34/34 [==============================] - 71s 2s/step - loss: 0.7039 - categorical_accuracy: 0.7360 - val_loss: 2.2026 - val_categorical_accuracy: 0.4100 - lr: 0.0010\nEpoch 16/30\n34/34 [==============================] - ETA: 0s - loss: 0.5129 - categorical_accuracy: 0.8190\nEpoch 16: saving model to model_init_2024-03-0411_42_54.002711/model-00016-0.51294-0.81900-2.57291-0.33000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.5129 - categorical_accuracy: 0.8190 - val_loss: 2.5729 - val_categorical_accuracy: 0.3300 - lr: 0.0010\nEpoch 17/30\n34/34 [==============================] - ETA: 0s - loss: 0.4920 - categorical_accuracy: 0.8130\nEpoch 17: saving model to model_init_2024-03-0411_42_54.002711/model-00017-0.49201-0.81297-1.29582-0.55000.h5\n34/34 [==============================] - 70s 2s/step - loss: 0.4920 - categorical_accuracy: 0.8130 - val_loss: 1.2958 - val_categorical_accuracy: 0.5500 - lr: 0.0010\nEpoch 18/30\n34/34 [==============================] - ETA: 0s - loss: 0.4520 - categorical_accuracy: 0.8446\nEpoch 18: saving model to model_init_2024-03-0411_42_54.002711/model-00018-0.45202-0.84465-1.68900-0.45000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.4520 - categorical_accuracy: 0.8446 - val_loss: 1.6890 - val_categorical_accuracy: 0.4500 - lr: 0.0010\nEpoch 19/30\n34/34 [==============================] - ETA: 0s - loss: 0.3806 - categorical_accuracy: 0.8597\nEpoch 19: saving model to model_init_2024-03-0411_42_54.002711/model-00019-0.38064-0.85973-1.11402-0.64000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.3806 - categorical_accuracy: 0.8597 - val_loss: 1.1140 - val_categorical_accuracy: 0.6400 - lr: 0.0010\nEpoch 20/30\n34/34 [==============================] - ETA: 0s - loss: 0.3680 - categorical_accuracy: 0.8718\nEpoch 20: saving model to model_init_2024-03-0411_42_54.002711/model-00020-0.36801-0.87179-1.99041-0.48000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.3680 - categorical_accuracy: 0.8718 - val_loss: 1.9904 - val_categorical_accuracy: 0.4800 - lr: 0.0010\nEpoch 21/30\n34/34 [==============================] - ETA: 0s - loss: 0.5364 - categorical_accuracy: 0.8024\nEpoch 21: saving model to model_init_2024-03-0411_42_54.002711/model-00021-0.53638-0.80241-2.12437-0.50000.h5\n34/34 [==============================] - 68s 2s/step - loss: 0.5364 - categorical_accuracy: 0.8024 - val_loss: 2.1244 - val_categorical_accuracy: 0.5000 - lr: 0.0010\nEpoch 22/30\n34/34 [==============================] - ETA: 0s - loss: 0.4295 - categorical_accuracy: 0.8326\nEpoch 22: saving model to model_init_2024-03-0411_42_54.002711/model-00022-0.42949-0.83258-3.24038-0.36000.h5\n34/34 [==============================] - 68s 2s/step - loss: 0.4295 - categorical_accuracy: 0.8326 - val_loss: 3.2404 - val_categorical_accuracy: 0.3600 - lr: 0.0010\nEpoch 23/30\n34/34 [==============================] - ETA: 0s - loss: 0.3869 - categorical_accuracy: 0.8597\nEpoch 23: saving model to model_init_2024-03-0411_42_54.002711/model-00023-0.38687-0.85973-1.82410-0.47000.h5\n34/34 [==============================] - 70s 2s/step - loss: 0.3869 - categorical_accuracy: 0.8597 - val_loss: 1.8241 - val_categorical_accuracy: 0.4700 - lr: 0.0010\nEpoch 24/30\n34/34 [==============================] - ETA: 0s - loss: 0.4450 - categorical_accuracy: 0.8326\nEpoch 24: saving model to model_init_2024-03-0411_42_54.002711/model-00024-0.44500-0.83258-1.26690-0.56000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.4450 - categorical_accuracy: 0.8326 - val_loss: 1.2669 - val_categorical_accuracy: 0.5600 - lr: 0.0010\nEpoch 25/30\n34/34 [==============================] - ETA: 0s - loss: 0.3500 - categorical_accuracy: 0.8763\nEpoch 25: saving model to model_init_2024-03-0411_42_54.002711/model-00025-0.35001-0.87632-1.01419-0.62000.h5\n34/34 [==============================] - 68s 2s/step - loss: 0.3500 - categorical_accuracy: 0.8763 - val_loss: 1.0142 - val_categorical_accuracy: 0.6200 - lr: 0.0010\nEpoch 26/30\n34/34 [==============================] - ETA: 0s - loss: 0.2726 - categorical_accuracy: 0.9020\nEpoch 26: saving model to model_init_2024-03-0411_42_54.002711/model-00026-0.27263-0.90196-0.69602-0.77000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.2726 - categorical_accuracy: 0.9020 - val_loss: 0.6960 - val_categorical_accuracy: 0.7700 - lr: 0.0010\nEpoch 27/30\n34/34 [==============================] - ETA: 0s - loss: 0.2320 - categorical_accuracy: 0.9246\nEpoch 27: saving model to model_init_2024-03-0411_42_54.002711/model-00027-0.23203-0.92459-0.63130-0.73000.h5\n34/34 [==============================] - 70s 2s/step - loss: 0.2320 - categorical_accuracy: 0.9246 - val_loss: 0.6313 - val_categorical_accuracy: 0.7300 - lr: 0.0010\nEpoch 28/30\n34/34 [==============================] - ETA: 0s - loss: 0.2110 - categorical_accuracy: 0.9321\nEpoch 28: saving model to model_init_2024-03-0411_42_54.002711/model-00028-0.21095-0.93213-0.69152-0.74000.h5\n34/34 [==============================] - 70s 2s/step - loss: 0.2110 - categorical_accuracy: 0.9321 - val_loss: 0.6915 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 29/30\n34/34 [==============================] - ETA: 0s - loss: 0.2044 - categorical_accuracy: 0.9276\nEpoch 29: saving model to model_init_2024-03-0411_42_54.002711/model-00029-0.20443-0.92760-0.64262-0.78000.h5\n34/34 [==============================] - 71s 2s/step - loss: 0.2044 - categorical_accuracy: 0.9276 - val_loss: 0.6426 - val_categorical_accuracy: 0.7800 - lr: 0.0010\nEpoch 30/30\n34/34 [==============================] - ETA: 0s - loss: 0.1663 - categorical_accuracy: 0.9487\nEpoch 30: saving model to model_init_2024-03-0411_42_54.002711/model-00030-0.16627-0.94872-0.88465-0.61000.h5\n34/34 [==============================] - 69s 2s/step - loss: 0.1663 - categorical_accuracy: 0.9487 - val_loss: 0.8847 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n<keras.src.callbacks.History at 0x7c84b07b76d0>","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:21:28.556242Z","iopub.execute_input":"2024-03-05T16:21:28.556653Z","iopub.status.idle":"2024-03-05T16:21:28.580884Z","shell.execute_reply.started":"2024-03-05T16:21:28.556619Z","shell.execute_reply":"2024-03-05T16:21:28.579988Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Model 6 - Batch size 20, epochs 30, drop out 0.4 and dense 64","metadata":{}},{"cell_type":"code","source":"batch_size = 20\nnum_epochs = 30\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:50.217153Z","iopub.execute_input":"2024-03-05T11:47:50.217484Z","iopub.status.idle":"2024-03-05T11:47:50.225510Z","shell.execute_reply.started":"2024-03-05T11:47:50.217455Z","shell.execute_reply":"2024-03-05T11:47:50.224528Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:50.226690Z","iopub.execute_input":"2024-03-05T11:47:50.227011Z","iopub.status.idle":"2024-03-05T11:47:50.236451Z","shell.execute_reply.started":"2024-03-05T11:47:50.226951Z","shell.execute_reply":"2024-03-05T11:47:50.235434Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model_6 = generate_model(batch_size, img_height, img_width, 0.4, 128)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:50.237998Z","iopub.execute_input":"2024-03-05T11:47:50.238344Z","iopub.status.idle":"2024-03-05T11:47:50.449127Z","shell.execute_reply.started":"2024-03-05T11:47:50.238314Z","shell.execute_reply":"2024-03-05T11:47:50.448385Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"optimiser = 'adam' #write your optimizer\nmodel_6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model_6.summary())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:47:50.450286Z","iopub.execute_input":"2024-03-05T11:47:50.450533Z","iopub.status.idle":"2024-03-05T11:47:50.512432Z","shell.execute_reply.started":"2024-03-05T11:47:50.450512Z","shell.execute_reply":"2024-03-05T11:47:50.511176Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv3d_20 (Conv3D)          (None, 20, 120, 120, 16   1312      \n                             )                                   \n                                                                 \n activation_20 (Activation)  (None, 20, 120, 120, 16   0         \n                             )                                   \n                                                                 \n batch_normalization_30 (Ba  (None, 20, 120, 120, 16   64        \n tchNormalization)           )                                   \n                                                                 \n max_pooling3d_20 (MaxPooli  (None, 10, 60, 60, 16)    0         \n ng3D)                                                           \n                                                                 \n conv3d_21 (Conv3D)          (None, 10, 60, 60, 32)    4128      \n                                                                 \n activation_21 (Activation)  (None, 10, 60, 60, 32)    0         \n                                                                 \n batch_normalization_31 (Ba  (None, 10, 60, 60, 32)    128       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_21 (MaxPooli  (None, 5, 30, 30, 32)     0         \n ng3D)                                                           \n                                                                 \n conv3d_22 (Conv3D)          (None, 5, 30, 30, 64)     16448     \n                                                                 \n activation_22 (Activation)  (None, 5, 30, 30, 64)     0         \n                                                                 \n batch_normalization_32 (Ba  (None, 5, 30, 30, 64)     256       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_22 (MaxPooli  (None, 2, 15, 15, 64)     0         \n ng3D)                                                           \n                                                                 \n conv3d_23 (Conv3D)          (None, 2, 15, 15, 128)    65664     \n                                                                 \n activation_23 (Activation)  (None, 2, 15, 15, 128)    0         \n                                                                 \n batch_normalization_33 (Ba  (None, 2, 15, 15, 128)    512       \n tchNormalization)                                               \n                                                                 \n max_pooling3d_23 (MaxPooli  (None, 1, 8, 8, 128)      0         \n ng3D)                                                           \n                                                                 \n flatten_5 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_15 (Dense)            (None, 128)               1048704   \n                                                                 \n batch_normalization_34 (Ba  (None, 128)               512       \n tchNormalization)                                               \n                                                                 \n dropout_10 (Dropout)        (None, 128)               0         \n                                                                 \n dense_16 (Dense)            (None, 128)               16512     \n                                                                 \n batch_normalization_35 (Ba  (None, 128)               512       \n tchNormalization)                                               \n                                                                 \n dropout_11 (Dropout)        (None, 128)               0         \n                                                                 \n dense_17 (Dense)            (None, 5)                 645       \n                                                                 \n=================================================================\nTotal params: 1155397 (4.41 MB)\nTrainable params: 1154405 (4.40 MB)\nNon-trainable params: 992 (3.88 KB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"#model_6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:38:51.266630Z","iopub.execute_input":"2024-03-04T14:38:51.266897Z","iopub.status.idle":"2024-03-04T15:15:52.887359Z","shell.execute_reply.started":"2024-03-04T14:38:51.266875Z","shell.execute_reply":"2024-03-04T15:15:52.886374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conv 2D + LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import mobilenet","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:54:03.334249Z","iopub.execute_input":"2024-03-05T11:54:03.334552Z","iopub.status.idle":"2024-03-05T11:54:03.341092Z","shell.execute_reply.started":"2024-03-05T11:54:03.334530Z","shell.execute_reply":"2024-03-05T11:54:03.340027Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 30\nbatch_size = 20","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:53:01.966176Z","iopub.execute_input":"2024-03-05T11:53:01.966508Z","iopub.status.idle":"2024-03-05T11:53:01.969996Z","shell.execute_reply.started":"2024-03-05T11:53:01.966484Z","shell.execute_reply":"2024-03-05T11:53:01.969307Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"mobilenet = mobilenet.MobileNet(weights='imagenet', include_top=False)\n\n\ntrain_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n    \n\nmodel7 = Sequential([\n    TimeDistributed(mobilenet, input_shape=(batch_size ,img_height, img_width,3))\n], name=\"mobilenet_lstm\")\n\nfor layer in model7.layers:\n    layer.trainable = False\n\nmodel7.add(TimeDistributed(BatchNormalization()))\nmodel7.add(TimeDistributed(MaxPooling2D(tuple([2]*2))))\nmodel7.add(TimeDistributed(Flatten()))\n\nmodel7.add(LSTM(256))\nmodel7.add(Dropout(0.2))\n\nmodel7.add(Dense(256,activation='relu'))\nmodel7.add(Dropout(0.2))\n\nmodel7.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:54:06.777328Z","iopub.execute_input":"2024-03-05T11:54:06.777622Z","iopub.status.idle":"2024-03-05T11:54:07.999749Z","shell.execute_reply.started":"2024-03-05T11:54:06.777602Z","shell.execute_reply":"2024-03-05T11:54:07.998902Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"model7.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint(model7.summary())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:54:18.961699Z","iopub.execute_input":"2024-03-05T11:54:18.962031Z","iopub.status.idle":"2024-03-05T11:54:19.008558Z","shell.execute_reply.started":"2024-03-05T11:54:18.962007Z","shell.execute_reply":"2024-03-05T11:54:19.007608Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Model: \"mobilenet_lstm\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n time_distributed_6 (TimeDi  (None, 20, 3, 3, 1024)    3228864   \n stributed)                                                      \n                                                                 \n time_distributed_7 (TimeDi  (None, 20, 3, 3, 1024)    4096      \n stributed)                                                      \n                                                                 \n time_distributed_8 (TimeDi  (None, 20, 1, 1, 1024)    0         \n stributed)                                                      \n                                                                 \n time_distributed_9 (TimeDi  (None, 20, 1024)          0         \n stributed)                                                      \n                                                                 \n lstm (LSTM)                 (None, 256)               1311744   \n                                                                 \n dropout_12 (Dropout)        (None, 256)               0         \n                                                                 \n dense_18 (Dense)            (None, 256)               65792     \n                                                                 \n dropout_13 (Dropout)        (None, 256)               0         \n                                                                 \n dense_19 (Dense)            (None, 5)                 1285      \n                                                                 \n=================================================================\nTotal params: 4611781 (17.59 MB)\nTrainable params: 1380869 (5.27 MB)\nNon-trainable params: 3230912 (12.32 MB)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"#model7_history = model7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n            callbacks=callbacks_list, validation_data=val_generator, \n            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T11:54:25.844896Z","iopub.execute_input":"2024-03-05T11:54:25.845240Z","iopub.status.idle":"2024-03-05T12:42:53.798280Z","shell.execute_reply.started":"2024-03-05T11:54:25.845217Z","shell.execute_reply":"2024-03-05T12:42:53.797091Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/55885017.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  model7_history = model7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n/tmp/ipykernel_33/515647629.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"Source path =  /kaggle/input/gesture-reconition/Project_data/train ; batch size = 20\nEpoch 1/30\n32/34 [===========================>..] - ETA: 6s - loss: 1.6575 - categorical_accuracy: 0.2078","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"34/34 [==============================] - ETA: 0s - loss: 1.6536 - categorical_accuracy: 0.2097Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 20\n\nEpoch 1: saving model to model_init_2024-03-0511_47_48.329730/model-00001-1.65357-0.20965-1.42314-0.56000.h5\n34/34 [==============================] - 124s 4s/step - loss: 1.6536 - categorical_accuracy: 0.2097 - val_loss: 1.4231 - val_categorical_accuracy: 0.5600 - lr: 0.0010\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"34/34 [==============================] - ETA: 0s - loss: 1.2528 - categorical_accuracy: 0.4932\nEpoch 2: saving model to model_init_2024-03-0511_47_48.329730/model-00002-1.25284-0.49321-1.12043-0.52000.h5\n34/34 [==============================] - 101s 3s/step - loss: 1.2528 - categorical_accuracy: 0.4932 - val_loss: 1.1204 - val_categorical_accuracy: 0.5200 - lr: 0.0010\nEpoch 3/30\n34/34 [==============================] - ETA: 0s - loss: 0.8916 - categorical_accuracy: 0.6576\nEpoch 3: saving model to model_init_2024-03-0511_47_48.329730/model-00003-0.89160-0.65762-0.93802-0.60000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.8916 - categorical_accuracy: 0.6576 - val_loss: 0.9380 - val_categorical_accuracy: 0.6000 - lr: 0.0010\nEpoch 4/30\n34/34 [==============================] - ETA: 0s - loss: 0.6509 - categorical_accuracy: 0.7692\nEpoch 4: saving model to model_init_2024-03-0511_47_48.329730/model-00004-0.65092-0.76923-0.89552-0.64000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.6509 - categorical_accuracy: 0.7692 - val_loss: 0.8955 - val_categorical_accuracy: 0.6400 - lr: 0.0010\nEpoch 5/30\n34/34 [==============================] - ETA: 0s - loss: 0.5587 - categorical_accuracy: 0.7934\nEpoch 5: saving model to model_init_2024-03-0511_47_48.329730/model-00005-0.55874-0.79336-0.84706-0.63000.h5\n34/34 [==============================] - 91s 3s/step - loss: 0.5587 - categorical_accuracy: 0.7934 - val_loss: 0.8471 - val_categorical_accuracy: 0.6300 - lr: 0.0010\nEpoch 6/30\n34/34 [==============================] - ETA: 0s - loss: 0.4528 - categorical_accuracy: 0.8326\nEpoch 6: saving model to model_init_2024-03-0511_47_48.329730/model-00006-0.45282-0.83258-0.83271-0.75000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.4528 - categorical_accuracy: 0.8326 - val_loss: 0.8327 - val_categorical_accuracy: 0.7500 - lr: 0.0010\nEpoch 7/30\n34/34 [==============================] - ETA: 0s - loss: 0.3826 - categorical_accuracy: 0.8507\nEpoch 7: saving model to model_init_2024-03-0511_47_48.329730/model-00007-0.38256-0.85068-0.98025-0.75000.h5\n34/34 [==============================] - 89s 3s/step - loss: 0.3826 - categorical_accuracy: 0.8507 - val_loss: 0.9802 - val_categorical_accuracy: 0.7500 - lr: 0.0010\nEpoch 8/30\n34/34 [==============================] - ETA: 0s - loss: 0.3788 - categorical_accuracy: 0.8658\nEpoch 8: saving model to model_init_2024-03-0511_47_48.329730/model-00008-0.37882-0.86576-0.89857-0.74000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.3788 - categorical_accuracy: 0.8658 - val_loss: 0.8986 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 9/30\n34/34 [==============================] - ETA: 0s - loss: 0.2979 - categorical_accuracy: 0.8929\nEpoch 9: saving model to model_init_2024-03-0511_47_48.329730/model-00009-0.29795-0.89291-0.86895-0.70000.h5\n34/34 [==============================] - 91s 3s/step - loss: 0.2979 - categorical_accuracy: 0.8929 - val_loss: 0.8690 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 10/30\n34/34 [==============================] - ETA: 0s - loss: 0.2245 - categorical_accuracy: 0.9216\nEpoch 10: saving model to model_init_2024-03-0511_47_48.329730/model-00010-0.22453-0.92157-1.03054-0.73000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.2245 - categorical_accuracy: 0.9216 - val_loss: 1.0305 - val_categorical_accuracy: 0.7300 - lr: 0.0010\nEpoch 11/30\n34/34 [==============================] - ETA: 0s - loss: 0.1593 - categorical_accuracy: 0.9563\nEpoch 11: saving model to model_init_2024-03-0511_47_48.329730/model-00011-0.15928-0.95626-0.81671-0.72000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.1593 - categorical_accuracy: 0.9563 - val_loss: 0.8167 - val_categorical_accuracy: 0.7200 - lr: 0.0010\nEpoch 12/30\n34/34 [==============================] - ETA: 0s - loss: 0.2377 - categorical_accuracy: 0.9080\nEpoch 12: saving model to model_init_2024-03-0511_47_48.329730/model-00012-0.23773-0.90799-0.99645-0.71000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.2377 - categorical_accuracy: 0.9080 - val_loss: 0.9965 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 13/30\n34/34 [==============================] - ETA: 0s - loss: 0.1370 - categorical_accuracy: 0.9653\nEpoch 13: saving model to model_init_2024-03-0511_47_48.329730/model-00013-0.13696-0.96531-1.08947-0.68000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.1370 - categorical_accuracy: 0.9653 - val_loss: 1.0895 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 14/30\n34/34 [==============================] - ETA: 0s - loss: 0.1445 - categorical_accuracy: 0.9593\nEpoch 14: saving model to model_init_2024-03-0511_47_48.329730/model-00014-0.14445-0.95928-1.01710-0.72000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.1445 - categorical_accuracy: 0.9593 - val_loss: 1.0171 - val_categorical_accuracy: 0.7200 - lr: 0.0010\nEpoch 15/30\n34/34 [==============================] - ETA: 0s - loss: 0.1185 - categorical_accuracy: 0.9578\nEpoch 15: saving model to model_init_2024-03-0511_47_48.329730/model-00015-0.11848-0.95777-1.12338-0.70000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.1185 - categorical_accuracy: 0.9578 - val_loss: 1.1234 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 16/30\n34/34 [==============================] - ETA: 0s - loss: 0.0814 - categorical_accuracy: 0.9713\nEpoch 16: saving model to model_init_2024-03-0511_47_48.329730/model-00016-0.08139-0.97134-1.28595-0.70000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.0814 - categorical_accuracy: 0.9713 - val_loss: 1.2859 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 17/30\n34/34 [==============================] - ETA: 0s - loss: 0.0559 - categorical_accuracy: 0.9789\nEpoch 17: saving model to model_init_2024-03-0511_47_48.329730/model-00017-0.05590-0.97888-1.21287-0.74000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.0559 - categorical_accuracy: 0.9789 - val_loss: 1.2129 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 18/30\n34/34 [==============================] - ETA: 0s - loss: 0.0677 - categorical_accuracy: 0.9744\nEpoch 18: saving model to model_init_2024-03-0511_47_48.329730/model-00018-0.06770-0.97436-0.95593-0.76000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.0677 - categorical_accuracy: 0.9744 - val_loss: 0.9559 - val_categorical_accuracy: 0.7600 - lr: 0.0010\nEpoch 19/30\n34/34 [==============================] - ETA: 0s - loss: 0.1202 - categorical_accuracy: 0.9623\nEpoch 19: saving model to model_init_2024-03-0511_47_48.329730/model-00019-0.12019-0.96229-1.36383-0.70000.h5\n34/34 [==============================] - 96s 3s/step - loss: 0.1202 - categorical_accuracy: 0.9623 - val_loss: 1.3638 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 20/30\n34/34 [==============================] - ETA: 0s - loss: 0.0716 - categorical_accuracy: 0.9819\nEpoch 20: saving model to model_init_2024-03-0511_47_48.329730/model-00020-0.07159-0.98190-0.94132-0.77000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.0716 - categorical_accuracy: 0.9819 - val_loss: 0.9413 - val_categorical_accuracy: 0.7700 - lr: 0.0010\nEpoch 21/30\n34/34 [==============================] - ETA: 0s - loss: 0.0767 - categorical_accuracy: 0.9744\nEpoch 21: saving model to model_init_2024-03-0511_47_48.329730/model-00021-0.07665-0.97436-1.04564-0.68000.h5\n34/34 [==============================] - 96s 3s/step - loss: 0.0767 - categorical_accuracy: 0.9744 - val_loss: 1.0456 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 22/30\n34/34 [==============================] - ETA: 0s - loss: 0.0671 - categorical_accuracy: 0.9759\nEpoch 22: saving model to model_init_2024-03-0511_47_48.329730/model-00022-0.06714-0.97587-1.18532-0.68000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.0671 - categorical_accuracy: 0.9759 - val_loss: 1.1853 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 23/30\n34/34 [==============================] - ETA: 0s - loss: 0.0488 - categorical_accuracy: 0.9819\nEpoch 23: saving model to model_init_2024-03-0511_47_48.329730/model-00023-0.04879-0.98190-1.46148-0.63000.h5\n34/34 [==============================] - 96s 3s/step - loss: 0.0488 - categorical_accuracy: 0.9819 - val_loss: 1.4615 - val_categorical_accuracy: 0.6300 - lr: 0.0010\nEpoch 24/30\n34/34 [==============================] - ETA: 0s - loss: 0.0834 - categorical_accuracy: 0.9698\nEpoch 24: saving model to model_init_2024-03-0511_47_48.329730/model-00024-0.08341-0.96983-1.14717-0.71000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.0834 - categorical_accuracy: 0.9698 - val_loss: 1.1472 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 25/30\n34/34 [==============================] - ETA: 0s - loss: 0.0627 - categorical_accuracy: 0.9759\nEpoch 25: saving model to model_init_2024-03-0511_47_48.329730/model-00025-0.06267-0.97587-1.12629-0.68000.h5\n34/34 [==============================] - 94s 3s/step - loss: 0.0627 - categorical_accuracy: 0.9759 - val_loss: 1.1263 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 26/30\n34/34 [==============================] - ETA: 0s - loss: 0.0813 - categorical_accuracy: 0.9744\nEpoch 26: saving model to model_init_2024-03-0511_47_48.329730/model-00026-0.08130-0.97436-1.22396-0.71000.h5\n34/34 [==============================] - 139s 4s/step - loss: 0.0813 - categorical_accuracy: 0.9744 - val_loss: 1.2240 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 27/30\n34/34 [==============================] - ETA: 0s - loss: 0.1307 - categorical_accuracy: 0.9442\nEpoch 27: saving model to model_init_2024-03-0511_47_48.329730/model-00027-0.13068-0.94419-1.35618-0.67000.h5\n34/34 [==============================] - 99s 3s/step - loss: 0.1307 - categorical_accuracy: 0.9442 - val_loss: 1.3562 - val_categorical_accuracy: 0.6700 - lr: 0.0010\nEpoch 28/30\n34/34 [==============================] - ETA: 0s - loss: 0.1425 - categorical_accuracy: 0.9472\nEpoch 28: saving model to model_init_2024-03-0511_47_48.329730/model-00028-0.14253-0.94721-1.27657-0.71000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.1425 - categorical_accuracy: 0.9472 - val_loss: 1.2766 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 29/30\n34/34 [==============================] - ETA: 0s - loss: 0.0571 - categorical_accuracy: 0.9789\nEpoch 29: saving model to model_init_2024-03-0511_47_48.329730/model-00029-0.05713-0.97888-1.18389-0.72000.h5\n34/34 [==============================] - 117s 3s/step - loss: 0.0571 - categorical_accuracy: 0.9789 - val_loss: 1.1839 - val_categorical_accuracy: 0.7200 - lr: 0.0010\nEpoch 30/30\n34/34 [==============================] - ETA: 0s - loss: 0.0648 - categorical_accuracy: 0.9759\nEpoch 30: saving model to model_init_2024-03-0511_47_48.329730/model-00030-0.06481-0.97587-1.05940-0.76000.h5\n34/34 [==============================] - 91s 3s/step - loss: 0.0648 - categorical_accuracy: 0.9759 - val_loss: 1.0594 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Source path =  /kaggle/input/gesture-reconition/Project_data/train ; batch size = 20\nEpoch 1/30\n32/34 [===========================>..] - ETA: 6s - loss: 1.6575 - categorical_accuracy: 0.2078\n/tmp/ipykernel_33/515647629.py:38: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n34/34 [==============================] - ETA: 0s - loss: 1.6536 - categorical_accuracy: 0.2097Source path =  /kaggle/input/gesture-reconition/Project_data/val ; batch size = 20\n\nEpoch 1: saving model to model_init_2024-03-0511_47_48.329730/model-00001-1.65357-0.20965-1.42314-0.56000.h5\n34/34 [==============================] - 124s 4s/step - loss: 1.6536 - categorical_accuracy: 0.2097 - val_loss: 1.4231 - val_categorical_accuracy: 0.5600 - lr: 0.0010\nEpoch 2/30\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n34/34 [==============================] - ETA: 0s - loss: 1.2528 - categorical_accuracy: 0.4932\nEpoch 2: saving model to model_init_2024-03-0511_47_48.329730/model-00002-1.25284-0.49321-1.12043-0.52000.h5\n34/34 [==============================] - 101s 3s/step - loss: 1.2528 - categorical_accuracy: 0.4932 - val_loss: 1.1204 - val_categorical_accuracy: 0.5200 - lr: 0.0010\nEpoch 3/30\n34/34 [==============================] - ETA: 0s - loss: 0.8916 - categorical_accuracy: 0.6576\nEpoch 3: saving model to model_init_2024-03-0511_47_48.329730/model-00003-0.89160-0.65762-0.93802-0.60000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.8916 - categorical_accuracy: 0.6576 - val_loss: 0.9380 - val_categorical_accuracy: 0.6000 - lr: 0.0010\nEpoch 4/30\n34/34 [==============================] - ETA: 0s - loss: 0.6509 - categorical_accuracy: 0.7692\nEpoch 4: saving model to model_init_2024-03-0511_47_48.329730/model-00004-0.65092-0.76923-0.89552-0.64000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.6509 - categorical_accuracy: 0.7692 - val_loss: 0.8955 - val_categorical_accuracy: 0.6400 - lr: 0.0010\nEpoch 5/30\n34/34 [==============================] - ETA: 0s - loss: 0.5587 - categorical_accuracy: 0.7934\nEpoch 5: saving model to model_init_2024-03-0511_47_48.329730/model-00005-0.55874-0.79336-0.84706-0.63000.h5\n34/34 [==============================] - 91s 3s/step - loss: 0.5587 - categorical_accuracy: 0.7934 - val_loss: 0.8471 - val_categorical_accuracy: 0.6300 - lr: 0.0010\nEpoch 6/30\n34/34 [==============================] - ETA: 0s - loss: 0.4528 - categorical_accuracy: 0.8326\nEpoch 6: saving model to model_init_2024-03-0511_47_48.329730/model-00006-0.45282-0.83258-0.83271-0.75000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.4528 - categorical_accuracy: 0.8326 - val_loss: 0.8327 - val_categorical_accuracy: 0.7500 - lr: 0.0010\nEpoch 7/30\n34/34 [==============================] - ETA: 0s - loss: 0.3826 - categorical_accuracy: 0.8507\nEpoch 7: saving model to model_init_2024-03-0511_47_48.329730/model-00007-0.38256-0.85068-0.98025-0.75000.h5\n34/34 [==============================] - 89s 3s/step - loss: 0.3826 - categorical_accuracy: 0.8507 - val_loss: 0.9802 - val_categorical_accuracy: 0.7500 - lr: 0.0010\nEpoch 8/30\n34/34 [==============================] - ETA: 0s - loss: 0.3788 - categorical_accuracy: 0.8658\nEpoch 8: saving model to model_init_2024-03-0511_47_48.329730/model-00008-0.37882-0.86576-0.89857-0.74000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.3788 - categorical_accuracy: 0.8658 - val_loss: 0.8986 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 9/30\n34/34 [==============================] - ETA: 0s - loss: 0.2979 - categorical_accuracy: 0.8929\nEpoch 9: saving model to model_init_2024-03-0511_47_48.329730/model-00009-0.29795-0.89291-0.86895-0.70000.h5\n34/34 [==============================] - 91s 3s/step - loss: 0.2979 - categorical_accuracy: 0.8929 - val_loss: 0.8690 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 10/30\n34/34 [==============================] - ETA: 0s - loss: 0.2245 - categorical_accuracy: 0.9216\nEpoch 10: saving model to model_init_2024-03-0511_47_48.329730/model-00010-0.22453-0.92157-1.03054-0.73000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.2245 - categorical_accuracy: 0.9216 - val_loss: 1.0305 - val_categorical_accuracy: 0.7300 - lr: 0.0010\nEpoch 11/30\n34/34 [==============================] - ETA: 0s - loss: 0.1593 - categorical_accuracy: 0.9563\nEpoch 11: saving model to model_init_2024-03-0511_47_48.329730/model-00011-0.15928-0.95626-0.81671-0.72000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.1593 - categorical_accuracy: 0.9563 - val_loss: 0.8167 - val_categorical_accuracy: 0.7200 - lr: 0.0010\nEpoch 12/30\n34/34 [==============================] - ETA: 0s - loss: 0.2377 - categorical_accuracy: 0.9080\nEpoch 12: saving model to model_init_2024-03-0511_47_48.329730/model-00012-0.23773-0.90799-0.99645-0.71000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.2377 - categorical_accuracy: 0.9080 - val_loss: 0.9965 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 13/30\n34/34 [==============================] - ETA: 0s - loss: 0.1370 - categorical_accuracy: 0.9653\nEpoch 13: saving model to model_init_2024-03-0511_47_48.329730/model-00013-0.13696-0.96531-1.08947-0.68000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.1370 - categorical_accuracy: 0.9653 - val_loss: 1.0895 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 14/30\n34/34 [==============================] - ETA: 0s - loss: 0.1445 - categorical_accuracy: 0.9593\nEpoch 14: saving model to model_init_2024-03-0511_47_48.329730/model-00014-0.14445-0.95928-1.01710-0.72000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.1445 - categorical_accuracy: 0.9593 - val_loss: 1.0171 - val_categorical_accuracy: 0.7200 - lr: 0.0010\nEpoch 15/30\n34/34 [==============================] - ETA: 0s - loss: 0.1185 - categorical_accuracy: 0.9578\nEpoch 15: saving model to model_init_2024-03-0511_47_48.329730/model-00015-0.11848-0.95777-1.12338-0.70000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.1185 - categorical_accuracy: 0.9578 - val_loss: 1.1234 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 16/30\n34/34 [==============================] - ETA: 0s - loss: 0.0814 - categorical_accuracy: 0.9713\nEpoch 16: saving model to model_init_2024-03-0511_47_48.329730/model-00016-0.08139-0.97134-1.28595-0.70000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.0814 - categorical_accuracy: 0.9713 - val_loss: 1.2859 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 17/30\n34/34 [==============================] - ETA: 0s - loss: 0.0559 - categorical_accuracy: 0.9789\nEpoch 17: saving model to model_init_2024-03-0511_47_48.329730/model-00017-0.05590-0.97888-1.21287-0.74000.h5\n34/34 [==============================] - 92s 3s/step - loss: 0.0559 - categorical_accuracy: 0.9789 - val_loss: 1.2129 - val_categorical_accuracy: 0.7400 - lr: 0.0010\nEpoch 18/30\n34/34 [==============================] - ETA: 0s - loss: 0.0677 - categorical_accuracy: 0.9744\nEpoch 18: saving model to model_init_2024-03-0511_47_48.329730/model-00018-0.06770-0.97436-0.95593-0.76000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.0677 - categorical_accuracy: 0.9744 - val_loss: 0.9559 - val_categorical_accuracy: 0.7600 - lr: 0.0010\nEpoch 19/30\n34/34 [==============================] - ETA: 0s - loss: 0.1202 - categorical_accuracy: 0.9623\nEpoch 19: saving model to model_init_2024-03-0511_47_48.329730/model-00019-0.12019-0.96229-1.36383-0.70000.h5\n34/34 [==============================] - 96s 3s/step - loss: 0.1202 - categorical_accuracy: 0.9623 - val_loss: 1.3638 - val_categorical_accuracy: 0.7000 - lr: 0.0010\nEpoch 20/30\n34/34 [==============================] - ETA: 0s - loss: 0.0716 - categorical_accuracy: 0.9819\nEpoch 20: saving model to model_init_2024-03-0511_47_48.329730/model-00020-0.07159-0.98190-0.94132-0.77000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.0716 - categorical_accuracy: 0.9819 - val_loss: 0.9413 - val_categorical_accuracy: 0.7700 - lr: 0.0010\nEpoch 21/30\n34/34 [==============================] - ETA: 0s - loss: 0.0767 - categorical_accuracy: 0.9744\nEpoch 21: saving model to model_init_2024-03-0511_47_48.329730/model-00021-0.07665-0.97436-1.04564-0.68000.h5\n34/34 [==============================] - 96s 3s/step - loss: 0.0767 - categorical_accuracy: 0.9744 - val_loss: 1.0456 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 22/30\n34/34 [==============================] - ETA: 0s - loss: 0.0671 - categorical_accuracy: 0.9759\nEpoch 22: saving model to model_init_2024-03-0511_47_48.329730/model-00022-0.06714-0.97587-1.18532-0.68000.h5\n34/34 [==============================] - 95s 3s/step - loss: 0.0671 - categorical_accuracy: 0.9759 - val_loss: 1.1853 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 23/30\n34/34 [==============================] - ETA: 0s - loss: 0.0488 - categorical_accuracy: 0.9819\nEpoch 23: saving model to model_init_2024-03-0511_47_48.329730/model-00023-0.04879-0.98190-1.46148-0.63000.h5\n34/34 [==============================] - 96s 3s/step - loss: 0.0488 - categorical_accuracy: 0.9819 - val_loss: 1.4615 - val_categorical_accuracy: 0.6300 - lr: 0.0010\nEpoch 24/30\n34/34 [==============================] - ETA: 0s - loss: 0.0834 - categorical_accuracy: 0.9698\nEpoch 24: saving model to model_init_2024-03-0511_47_48.329730/model-00024-0.08341-0.96983-1.14717-0.71000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.0834 - categorical_accuracy: 0.9698 - val_loss: 1.1472 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 25/30\n34/34 [==============================] - ETA: 0s - loss: 0.0627 - categorical_accuracy: 0.9759\nEpoch 25: saving model to model_init_2024-03-0511_47_48.329730/model-00025-0.06267-0.97587-1.12629-0.68000.h5\n34/34 [==============================] - 94s 3s/step - loss: 0.0627 - categorical_accuracy: 0.9759 - val_loss: 1.1263 - val_categorical_accuracy: 0.6800 - lr: 0.0010\nEpoch 26/30\n34/34 [==============================] - ETA: 0s - loss: 0.0813 - categorical_accuracy: 0.9744\nEpoch 26: saving model to model_init_2024-03-0511_47_48.329730/model-00026-0.08130-0.97436-1.22396-0.71000.h5\n34/34 [==============================] - 139s 4s/step - loss: 0.0813 - categorical_accuracy: 0.9744 - val_loss: 1.2240 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 27/30\n34/34 [==============================] - ETA: 0s - loss: 0.1307 - categorical_accuracy: 0.9442\nEpoch 27: saving model to model_init_2024-03-0511_47_48.329730/model-00027-0.13068-0.94419-1.35618-0.67000.h5\n34/34 [==============================] - 99s 3s/step - loss: 0.1307 - categorical_accuracy: 0.9442 - val_loss: 1.3562 - val_categorical_accuracy: 0.6700 - lr: 0.0010\nEpoch 28/30\n34/34 [==============================] - ETA: 0s - loss: 0.1425 - categorical_accuracy: 0.9472\nEpoch 28: saving model to model_init_2024-03-0511_47_48.329730/model-00028-0.14253-0.94721-1.27657-0.71000.h5\n34/34 [==============================] - 93s 3s/step - loss: 0.1425 - categorical_accuracy: 0.9472 - val_loss: 1.2766 - val_categorical_accuracy: 0.7100 - lr: 0.0010\nEpoch 29/30\n34/34 [==============================] - ETA: 0s - loss: 0.0571 - categorical_accuracy: 0.9789\nEpoch 29: saving model to model_init_2024-03-0511_47_48.329730/model-00029-0.05713-0.97888-1.18389-0.72000.h5\n34/34 [==============================] - 117s 3s/step - loss: 0.0571 - categorical_accuracy: 0.9789 - val_loss: 1.1839 - val_categorical_accuracy: 0.7200 - lr: 0.0010\nEpoch 30/30\n34/34 [==============================] - ETA: 0s - loss: 0.0648 - categorical_accuracy: 0.9759\nEpoch 30: saving model to model_init_2024-03-0511_47_48.329730/model-00030-0.06481-0.97587-1.05940-0.76000.h5\n34/34 [==============================] - 91s 3s/step - loss: 0.0648 - categorical_accuracy: 0.9759 - val_loss: 1.0594 - val_categorical_accuracy: 0.7600 - lr: 0.0010","metadata":{}},{"cell_type":"markdown","source":"**The best Validation accuracy we see is 77%**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}